"""
å®Œæ•´é›†æˆçš„åŠ¨æ€åˆ†ç‰‡ç³»ç»Ÿ - çœŸå®çš„å››æ­¥æµæ°´çº¿
ä½¿ç”¨40ä¸ªçœŸå®å­—æ®µï¼Œå¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ ï¼ŒEvolveGCNï¼Œå’Œç»Ÿä¸€åé¦ˆå¼•æ“

é›†æˆåˆ°BlockEmulatorçš„å®Œæ•´åˆ†ç‰‡ç³»ç»Ÿ
"""
import sys
import os
import json
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import pickle
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict
import logging
import time

# å¯¼å…¥å¼‚æ„å›¾æ„å»ºå™¨
try:
    from partition.feature.graph_builder import HeterogeneousGraphBuilder
except ImportError:
    try:
        from feature.graph_builder import HeterogeneousGraphBuilder
    except ImportError:
        HeterogeneousGraphBuilder = None

# è®¾ç½®UTF-8ç¼–ç 
import locale
try:
    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
except:
    try:
        locale.setlocale(locale.LC_ALL, 'C.UTF-8')
    except:
        pass

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('complete_integrated_sharding.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class CompleteIntegratedShardingSystem:
    """å®Œæ•´é›†æˆçš„åŠ¨æ€åˆ†ç‰‡ç³»ç»Ÿ"""
    
    def __init__(self, config_file: str = "python_config.json", device: str = None):
        """
        åˆå§‹åŒ–å®Œæ•´é›†æˆåˆ†ç‰‡ç³»ç»Ÿ
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ ('cuda', 'cpu', 'auto')
        """
        #  [OPTIMIZATION] çœŸå®40å­—æ®µé…ç½® + åŠ¨æ€f_classicç»´åº¦è®¡ç®—
        # åŸºäºcommittee_evolvegcn.goçš„extractRealStaticFeatureså’ŒextractRealDynamicFeatures
        self.real_feature_dims = {
            'hardware': 11,           # ç¡¬ä»¶ç‰¹å¾ï¼ˆé™æ€ï¼‰ - CPU(2) + Memory(3) + Storage(3) + Network(3)
            'network_topology': 5,    # ç½‘ç»œæ‹“æ‰‘ç‰¹å¾ï¼ˆé™æ€ï¼‰ - intra_shard_conn + inter_shard_conn + weighted_degree + active_conn + adaptability
            'heterogeneous_type': 2,  # å¼‚æ„ç±»å‹ç‰¹å¾ï¼ˆé™æ€ï¼‰ - node_type + core_eligibility  
            'onchain_behavior': 15,   # é“¾ä¸Šè¡Œä¸ºç‰¹å¾ï¼ˆåŠ¨æ€ï¼‰ - transaction(2) + cross_shard(2) + block_gen(2) + tx_types(2) + consensus(3) + resource(3) + network_dynamic(3)
            'dynamic_attributes': 7   # åŠ¨æ€å±æ€§ç‰¹å¾ï¼ˆåŠ¨æ€ï¼‰ - tx_processing(2) + application(3)
        }
        
        #  [DATA_FLOW] åŠ¨æ€è®¡ç®—å®é™…f_classicç»´åº¦
        self.base_feature_dim = sum(self.real_feature_dims.values())  # 40ç»´åŸºç¡€ç‰¹å¾
        # é€šè¿‡ç‰¹å¾å·¥ç¨‹æ‰©å±•åˆ°åˆç†çš„f_classicç»´åº¦ï¼ˆå¦‚64ç»´ã€80ç»´æˆ–96ç»´ï¼‰
        # é¿å…è¿‡åº¦æŠ•å½±åˆ°128ç»´é€ æˆçš„ä¿¡æ¯ç¨€é‡Š
        self.f_classic_dim = self._calculate_optimal_f_classic_dim()  # è®¡ç®—å¹¶è®¾ç½®æœ€ä¼˜ç»´åº¦
        
        # è®¾ç½®è®¡ç®—è®¾å¤‡
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_file)
        
        # åˆå§‹åŒ–å¼‚æ„å›¾æ„å»ºå™¨
        if HeterogeneousGraphBuilder is not None:
            self.heterogeneous_graph_builder = HeterogeneousGraphBuilder()
            logger.info("HeterogeneousGraphBuilder åˆå§‹åŒ–æˆåŠŸ")
        else:
            self.heterogeneous_graph_builder = None
            logger.error("HeterogeneousGraphBuilder å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•æ„å»ºæ­£ç¡®çš„å¼‚æ„å›¾")
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path("complete_integrated_output")
        self.output_dir.mkdir(exist_ok=True)
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.step1_processor = None
        self.step2_processor = None  
        self.step3_processor = None
        self.step4_processor = None
        
        logger.info(f"å®Œæ•´é›†æˆåˆ†ç‰‡ç³»ç»Ÿåˆå§‹åŒ–")
        logger.info(f"è®¾å¤‡: {self.device}")
        logger.info(f"çœŸå®ç‰¹å¾ç»´åº¦: {sum(self.real_feature_dims.values())} (40å­—æ®µ)")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
    def _calculate_optimal_f_classic_dim(self):
        """
         [DATA_FLOW] åŸºäºå®é™…æ•°æ®å­—æ®µè®¡ç®—æœ€ä¼˜f_classicç»´åº¦
        
        æ ¹æ®40ä¸ªçœŸå®å­—æ®µè®¡ç®—åˆç†çš„f_classicç»´åº¦ï¼Œé¿å…è¿‡åº¦æŠ•å½±é€ æˆä¿¡æ¯ç¨€é‡Š
        """
        base_dim = self.base_feature_dim  # 40ç»´
        
        # é€‰æ‹©åˆé€‚çš„æ‰©å±•å€æ•°ï¼Œä¿æŒä¿¡æ¯å¯†åº¦
        # 1.5x (60ç»´)ã€2x (80ç»´)ã€2.4x (96ç»´) éƒ½æ¯”3.2x (128ç»´) æ›´åˆç†
        expansion_options = {
            60: 1.5,   # è½»é‡æ‰©å±•
            80: 2.0,   # æ ‡å‡†æ‰©å±•
            96: 2.4,   # é«˜çº§æ‰©å±•
            128: 3.2   # åŸå§‹è®¾ç½®ï¼ˆå¯èƒ½è¿‡åº¦æ‰©å±•ï¼‰
        }
        
        # åŸºäºç³»ç»Ÿå¤æ‚åº¦é€‰æ‹© - åˆ†ç‰‡ç³»ç»Ÿå»ºè®®ä½¿ç”¨80ç»´
        optimal_dim = 80
        
        logger.info(f" [DATA_FLOW] f_classicç»´åº¦é€‰æ‹©: {base_dim}ç»´ â†’ {optimal_dim}ç»´ (æ‰©å±•å€æ•°: {optimal_dim/base_dim:.1f}x)")
        
        return optimal_dim
    
    def _load_config(self, config_file: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if os.path.exists(config_file):
                # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼
                for encoding in ['utf-8-sig', 'utf-8', 'gbk']:
                    try:
                        with open(config_file, 'r', encoding=encoding) as f:
                            config = json.load(f)
                        logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_file} (ç¼–ç : {encoding})")
                        
                        # å°†ç°æœ‰é…ç½®è½¬æ¢ä¸ºæˆ‘ä»¬éœ€è¦çš„æ ¼å¼
                        return self._convert_config_format(config)
                    except (UnicodeDecodeError, json.JSONDecodeError):
                        continue
                        
        except Exception as e:
            logger.warning(f"ğŸ“‹ [CONFIG] åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            logger.warning("ğŸ“‹ [CONFIG] ä½¿ç”¨é»˜è®¤é…ç½®ç»§ç»­è¿è¡Œï¼Œè¿™æ˜¯æ­£å¸¸çš„ç‹¬ç«‹è¿è¡Œæ¨¡å¼")
        
        # è¿”å›é»˜è®¤é…ç½®
        logger.info("ä½¿ç”¨é»˜è®¤é…ç½®")
        return self._get_default_config()
    
    def _convert_config_format(self, original_config: Dict[str, Any]) -> Dict[str, Any]:
        """å°†åŸæœ‰é…ç½®æ ¼å¼è½¬æ¢ä¸ºæ–°æ ¼å¼"""
        return {
            "step1": {
                "feature_dims": self.real_feature_dims,
                "normalize": True,
                "validate": True
            },
            "step2": {
                "embed_dim": 64,
                "temperature": 0.1,
                "num_epochs": original_config.get("epochs_per_iteration", 50),
                "learning_rate": 0.001
            },
            "step3": {
                "hidden_dim": 128,
                "num_timesteps": 10,
                "num_epochs": original_config.get("epochs_per_iteration", 100),
                "learning_rate": 0.001
            },
            "step4": {
                "feedback_weight": 1.0,
                "evolution_threshold": 0.1,
                "max_history": 100,
                "learning_rate": 0.01,
                "weight_decay": 1e-4,
                "feedback_weights": {
                    "balance": 0.35,
                    "cross_shard": 0.25,
                    "security": 0.20,
                    "consensus": 0.20
                }
            },
            # ä¿ç•™åŸæœ‰é…ç½®
            "original": original_config
        }
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "step1": {
                "feature_dims": self.real_feature_dims,
                "normalize": True,
                "validate": True
            },
            "step2": {
                "embed_dim": 64,
                "temperature": 0.1,
                "num_epochs": 50,
                "learning_rate": 0.001,
                "hidden_dim": 64,
                "time_dim": 16,
                "k_ratio": 0.9,
                "alpha": 0.3,
                "beta": 0.4,
                "gamma": 0.3,
                "tau": 0.09,
                "num_node_types": 5,
                "num_edge_types": 3
            },
            "step3": {
                "hidden_dim": 128,
                "num_timesteps": 10,
                "num_epochs": 100,
                "learning_rate": 0.001,
                "num_shards": 8
            },
            "step4": {
                "feedback_weight": 1.0,
                "evolution_threshold": 0.1,
                "max_history": 100,
                "learning_rate": 0.01,
                "weight_decay": 1e-4,
                "feedback_weights": {
                    "balance": 0.35,
                    "cross_shard": 0.25,
                    "security": 0.20,
                    "consensus": 0.20
                }
            }
        }
    
    def initialize_step1(self):
        """åˆå§‹åŒ–ç¬¬ä¸€æ­¥ï¼šç‰¹å¾æå–"""
        logger.info("åˆå§‹åŒ–Step1ï¼šç‰¹å¾æå–")
        
        try:
            # ç¡®ä¿è®¾å¤‡å·²åˆå§‹åŒ–
            if not hasattr(self, 'device'):
                self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
                logger.warning(f"è®¾å¤‡æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡: {self.device}")
            
            # ç¡®ä¿æ­£ç¡®å¯¼å…¥è·¯å¾„
            import sys
            root_step1_path = str(Path(__file__).parent / "partition" / "feature")
            if root_step1_path not in sys.path:
                sys.path.insert(0, root_step1_path)

            # å¯¼å…¥æ ¸å¿ƒæ¨¡å—
            from blockemulator_adapter import BlockEmulatorAdapter
            
            # åˆ›å»ºç®€åŒ–çš„ç‰¹å¾æå–å™¨
            self.step1_processor = self._create_simple_step1_processor()
            
            logger.info("Step1ç‰¹å¾æå–å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"Step1åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            # å¿…é¡»ä½¿ç”¨çœŸå®å®ç°
            raise RuntimeError(f"Step1åˆå§‹åŒ–å¤±è´¥ï¼Œå¿…é¡»ä½¿ç”¨çœŸå®å®ç°: {e}")

    def _create_simple_step1_processor(self):
        """åˆ›å»ºç®€åŒ–çš„Step1å¤„ç†å™¨"""
        
        # ç¡®ä¿è®¾å¤‡å±æ€§å·²è®¾ç½®
        if not hasattr(self, 'device'):
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.warning(f"_create_simple_step1_processorä¸­è®¾å¤‡æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡: {self.device}")
        
        class SimpleStep1Processor:
            def __init__(self, parent):
                self.parent = parent
                self.feature_dims = parent.real_feature_dims
                
                # å®‰å…¨åœ°è®¿é—®è®¾å¤‡å±æ€§
                if hasattr(parent, 'device'):
                    self.device = parent.device
                else:
                    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
                    logger.warning(f"çˆ¶å¯¹è±¡æœªè®¾ç½®è®¾å¤‡ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡: {self.device}")
                
                # ç»§æ‰¿f_classic_dimå±æ€§
                self.f_classic_dim = parent.f_classic_dim
                
                # å¯¼å…¥é€‚é…å™¨
                try:
                    from blockemulator_adapter import BlockEmulatorAdapter
                    self.adapter = BlockEmulatorAdapter()
                    logger.info("BlockEmulatorAdapteråˆå§‹åŒ–æˆåŠŸ")
                    
                    # ä½¿ç”¨ä¼˜åŒ–çš„MainPipeline
                    try:
                        from partition.feature.MainPipeline import Pipeline
                        # ä½¿ç”¨ä¼˜åŒ–å‚æ•°ï¼šè·³è¿‡f_fusedç”Ÿæˆï¼ŒèŠ‚çœè®¡ç®—å¼€é”€
                        self.extractor = Pipeline(use_fusion=False, save_adjacency=True, skip_fused=True)
                        logger.info(" [OPTIMIZATION] Pipelineåˆå§‹åŒ–æˆåŠŸ - å·²è·³è¿‡f_fusedç”Ÿæˆï¼Œä»…ä¿ç•™f_classic+adjacency_matrix")
                    except ImportError as e:
                        logger.warning(f" [FALLBACK] Pipelineå¯¼å…¥å¤±è´¥: {e}, ä½¿ç”¨å¤‡ç”¨ç‰¹å¾æå–å™¨")
                        # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨é€‚é…å™¨ä¸­çš„ç‰¹å¾æå–å™¨
                        if hasattr(self.adapter, 'comprehensive_extractor'):
                            self.extractor = self.adapter.comprehensive_extractor
                            logger.info("ComprehensiveFeatureExtractorå¼•ç”¨è®¾ç½®æˆåŠŸ")
                        else:
                            logger.warning("é€‚é…å™¨ä¸­æ²¡æœ‰comprehensive_extractor")
                            self.extractor = None
                    
                except Exception as e:
                    logger.error(f"BlockEmulatorAdapteråˆå§‹åŒ–å¤±è´¥: {e}")
                    raise
                
            def extract_real_features(self, node_data=None, feature_dims=None):
                """
                ä½¿ç”¨çœŸå®ç‰¹å¾æå–å™¨ä»node_dataä¸­æå–ç‰¹å¾
                
                Args:
                    node_data: æ¥è‡ªGoæ¥å£æˆ–BlockEmulatorçš„çœŸå®èŠ‚ç‚¹æ•°æ®
                    feature_dims: ç‰¹å¾ç»´åº¦é…ç½®
                
                Returns:
                    åŒ…å«çœŸå®ç‰¹å¾çš„å­—å…¸
                """
                try:
                    logger.info("=== å¼€å§‹çœŸå®ç‰¹å¾æå– ===")
                    
                    # å¤„ç†è¾“å…¥æ•°æ®
                    if node_data is None:
                        logger.warning("âš ï¸  [FALLBACK] node_dataä¸ºç©ºï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œæ¼”ç¤º")
                        logger.warning("âš ï¸  [FALLBACK] è¿™æ˜¯æµ‹è¯•æ”¯æŒæœºåˆ¶ï¼Œç”Ÿäº§ç¯å¢ƒåº”æä¾›çœŸå®èŠ‚ç‚¹æ•°æ®")
                        # åˆ›å»ºåŸºæœ¬çš„æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
                        node_data = self._create_basic_test_data()
                    
                    # è§£æä¸åŒæ ¼å¼çš„è¾“å…¥æ•°æ®
                    processed_nodes = self._parse_input_data(node_data)
                    
                    logger.info(f"è§£æå¾—åˆ° {len(processed_nodes)} ä¸ªèŠ‚ç‚¹")
                    
                    # æå–åŸå§‹èŠ‚ç‚¹æ˜ å°„ä¿¡æ¯
                    original_node_mapping = self._extract_original_node_mapping(processed_nodes)
                    
                    # ä½¿ç”¨çœŸå®ç‰¹å¾æå–å™¨å¤„ç†ï¼ˆåŒæ ¼å¼è¾“å‡ºï¼‰
                    result = self._extract_using_real_extractor(processed_nodes)
                    
                    if result is None:
                        logger.error("âŒ [STEP1] çœŸå®ç‰¹å¾æå–å¤±è´¥")
                        return None
                    
                    #  [NODE_MAPPING] æ·»åŠ èŠ‚ç‚¹æ˜ å°„ä¿¡æ¯åˆ°ç»“æœä¸­ï¼ŒGoæ¥å£ä¼šéœ€è¦è¿™äº›ä¿¡æ¯
                    if 'metadata' not in result:
                        result['metadata'] = {}
                    
                    result['metadata']['node_mapping'] = original_node_mapping
                    result['node_info'] = original_node_mapping  # Goæ¥å£å…¼å®¹æ€§å­—æ®µ
                    
                    logger.info(f" [STEP1] èŠ‚ç‚¹æ˜ å°„ä¿¡æ¯å·²ä¿å­˜: {len(original_node_mapping.get('original_node_keys', []))}ä¸ªèŠ‚ç‚¹")
                    logger.info(f" [NODE_MAPPING] å‰3ä¸ªèŠ‚ç‚¹é”®: {original_node_mapping.get('original_node_keys', [])[:3]}")
                    
                    return result
                    
                except Exception as e:
                    logger.error(f"çœŸå®ç‰¹å¾æå–å¤±è´¥: {e}")
                    raise
            
            def _parse_input_data(self, node_data):
                """è§£æè¾“å…¥çš„èŠ‚ç‚¹æ•°æ®"""
                try:
                    processed_nodes = []
                    
                    # æƒ…å†µ1ï¼šæ¥è‡ªGoæ¥å£çš„æ ¼å¼ (åŒ…å«nodesåˆ—è¡¨)
                    if isinstance(node_data, dict) and 'nodes' in node_data:
                        logger.info("æ£€æµ‹åˆ°Goæ¥å£æ ¼å¼çš„æ•°æ® (nodes)")
                        nodes_list = node_data['nodes']
                        
                        for node_info in nodes_list:
                            processed_node = self._convert_go_node_to_real_format(node_info)
                            processed_nodes.append(processed_node)
                    
                    # æƒ…å†µ1bï¼šæ¥è‡ªGoæ¥å£çš„æ ¼å¼ (åŒ…å«node_featuresåˆ—è¡¨) - æ–°å¢
                    elif isinstance(node_data, dict) and 'node_features' in node_data:
                        logger.info("æ£€æµ‹åˆ°Goæ¥å£æ ¼å¼çš„æ•°æ® (node_features)")
                        nodes_list = node_data['node_features']
                        
                        for node_info in nodes_list:
                            processed_node = self._convert_go_node_to_real_format(node_info)
                            processed_nodes.append(processed_node)
                    
                    # æƒ…å†µ2ï¼šç›´æ¥çš„èŠ‚ç‚¹åˆ—è¡¨
                    elif isinstance(node_data, list):
                        logger.info("æ£€æµ‹åˆ°èŠ‚ç‚¹åˆ—è¡¨æ ¼å¼çš„æ•°æ®")
                        
                        for node_info in node_data:
                            if isinstance(node_info, dict):
                                # æ£€æŸ¥æ˜¯å¦æ˜¯Goæ¥å£æ ¼å¼çš„èŠ‚ç‚¹
                                if 'node_id' in node_info and isinstance(node_info.get('node_id'), str) and node_info['node_id'].startswith('S'):
                                    processed_node = self._convert_go_node_to_real_format(node_info)
                                else:
                                    processed_node = self._convert_dict_node_to_real_format(node_info)
                                processed_nodes.append(processed_node)
                            else:
                                # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œåˆ›å»ºåŸºæœ¬èŠ‚ç‚¹
                                processed_node = self._create_basic_node(len(processed_nodes))
                                processed_nodes.append(processed_node)
                    
                    # æƒ…å†µ3ï¼šå•ä¸ªå­—å…¸ (ä½†ä¸æ˜¯Goæ¥å£æ ¼å¼)
                    elif isinstance(node_data, dict):
                        logger.info("æ£€æµ‹åˆ°å•ä¸ªå­—å…¸æ ¼å¼çš„æ•°æ®")
                        processed_node = self._convert_dict_node_to_real_format(node_data)
                        processed_nodes.append(processed_node)
                    
                    # æƒ…å†µ4ï¼šå…¶ä»–æ ¼å¼ï¼Œåˆ›å»ºæµ‹è¯•æ•°æ®
                    else:
                        logger.warning(f"æœªè¯†åˆ«çš„æ•°æ®æ ¼å¼: {type(node_data)}ï¼Œåˆ›å»ºæµ‹è¯•æ•°æ®")
                        processed_nodes = self._create_basic_test_data()
                    
                    return processed_nodes
                    
                except Exception as e:
                    logger.error(f"æ•°æ®è§£æå¤±è´¥: {e}")
                    # è¿”å›åŸºæœ¬æµ‹è¯•æ•°æ®
                    return self._create_basic_test_data()
            
            def _extract_original_node_mapping(self, processed_nodes):
                """æå–åŸå§‹èŠ‚ç‚¹æ˜ å°„ä¿¡æ¯ï¼Œä¿å­˜çœŸå®çš„S{ShardID}N{NodeID}æ ¼å¼NodeID"""
                try:
                    node_info = {
                        'node_ids': [],  # ä¿å­˜çœŸå®çš„S{ShardID}N{NodeID}æ ¼å¼
                        'shard_ids': [],
                        'original_node_keys': [],  # ä¿å­˜åŸå§‹çš„èŠ‚ç‚¹é”®
                        'timestamps': []
                    }
                    
                    for i, node in enumerate(processed_nodes):
                        original_node_key = None
                        shard_id = None
                        node_id = None
                        
                        if hasattr(node, 'ShardID') and hasattr(node, 'NodeID'):
                            shard_id = node.ShardID
                            node_id = node.NodeID
                            # ä»Goç³»ç»Ÿä¼ é€’çš„NodeIDå­—æ®µå¯èƒ½å·²ç»æ˜¯S{ShardID}N{NodeID}æ ¼å¼
                            if isinstance(node.NodeID, str) and node.NodeID.startswith('S') and 'N' in node.NodeID:
                                original_node_key = node.NodeID
                            else:
                                original_node_key = f"S{shard_id}N{node_id}"
                        elif isinstance(node, dict):
                            shard_id = node.get('ShardID', node.get('shard_id', None))
                            if shard_id is None:
                                logger.error(f"âŒ [ERROR] å­—å…¸æ ¼å¼èŠ‚ç‚¹{i}ç¼ºå°‘ShardIDä¿¡æ¯: {node}")
                                raise ValueError(f"èŠ‚ç‚¹{i}ç¼ºå°‘ShardIDï¼Œä¸èƒ½ä½¿ç”¨å¤‡ç”¨å€¼")
                            node_id_raw = node.get('NodeID', node.get('node_id', i))
                            
                            # æ£€æŸ¥node_idæ˜¯å¦å·²ç»æ˜¯S{ShardID}N{NodeID}æ ¼å¼
                            if isinstance(node_id_raw, str) and node_id_raw.startswith('S') and 'N' in node_id_raw:
                                original_node_key = node_id_raw
                                # ä»S{ShardID}N{NodeID}æ ¼å¼ä¸­æå–å®é™…çš„node_id
                                try:
                                    if 'N' in node_id_raw:
                                        node_id = int(node_id_raw.split('N')[1])
                                    else:
                                        node_id = i
                                except:
                                    node_id = i
                            else:
                                node_id = int(node_id_raw) if isinstance(node_id_raw, (int, str)) else i
                                original_node_key = f"S{shard_id}N{node_id}"
                        else:
                            # ğŸš« ä¿®æ­£: ä¸å†ä½¿ç”¨å›ºå®šçš„å¤‡ç”¨å®ç°ï¼Œè¦æ±‚çœŸå®æ•°æ®
                            logger.error(f"âŒ [ERROR] ç¬¬{i}ä¸ªèŠ‚ç‚¹ç¼ºå°‘çœŸå®çš„ShardIDå’ŒNodeIDä¿¡æ¯")
                            logger.error(f"âŒ [ERROR] processed_nodes[{i}]: {node}")
                            logger.error(f"âŒ [ERROR] æ— æ³•ç»§ç»­å¤„ç†ï¼Œéœ€è¦ä»BlockEmulatorè·å–çœŸå®çš„èŠ‚ç‚¹æ•°æ®")
                            raise ValueError(f"èŠ‚ç‚¹{i}ç¼ºå°‘çœŸå®çš„ShardIDå’ŒNodeIDï¼Œä¸èƒ½ä½¿ç”¨å›ºå®šå¤‡ç”¨å€¼")
                        
                        node_info['shard_ids'].append(shard_id)
                        node_info['node_ids'].append(node_id)
                        node_info['original_node_keys'].append(original_node_key)
                        node_info['timestamps'].append(int(time.time()) + i)
                    
                    logger.info(f"æå–åˆ°åŸå§‹èŠ‚ç‚¹æ˜ å°„ä¿¡æ¯ï¼š{len(node_info['shard_ids'])}ä¸ªèŠ‚ç‚¹")
                    logger.info(f"å‰3ä¸ªèŠ‚ç‚¹çš„æ˜ å°„: {node_info['original_node_keys'][:3]}")
                    return node_info
                    
                except Exception as e:
                    logger.error(f"æå–åŸå§‹èŠ‚ç‚¹æ˜ å°„å¤±è´¥: {e}")
                    logger.error(f"ğŸš« [ERROR] ä¸åº”è¯¥ä½¿ç”¨å¤‡ç”¨æ˜ å°„ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼")
                    # æ‹’ç»ä½¿ç”¨ç®€åŒ–çš„å¤‡ç”¨æ˜ å°„ï¼Œå¼ºåˆ¶ä½¿ç”¨çœŸå®æ•°æ®
                    raise ValueError(f"èŠ‚ç‚¹æ˜ å°„æå–å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæ­£ç¡®çš„NodeIDæ ¼å¼: {e}")
                    # æ³¨é‡Šæ‰é”™è¯¯çš„å¤‡ç”¨å®ç°
                    # num_nodes = len(processed_nodes) if processed_nodes else 10
                    # return {
                    #     'node_ids': [i for i in range(num_nodes)],
                    #     'shard_ids': [i % 4 for i in range(num_nodes)],
                    #     'original_node_keys': [f"S{i % 4}N{i}" for i in range(num_nodes)],
                    #     'timestamps': [int(time.time()) + i for i in range(num_nodes)]
                    # }
            
            def _convert_go_node_to_real_format(self, go_node_info):
                """å°†Goæ¥å£çš„èŠ‚ç‚¹ä¿¡æ¯è½¬æ¢ä¸ºçœŸå®ç‰¹å¾æå–å™¨å¯ç”¨çš„æ ¼å¼"""
                logger.debug(f" [GO_INTERFACE] è½¬æ¢GoèŠ‚ç‚¹æ•°æ®: {go_node_info.get('node_id', 'unknown')}")
                
                # ä»Goæ¥å£æå–çœŸå®çš„èŠ‚ç‚¹IDå’Œåˆ†ç‰‡ä¿¡æ¯
                node_id_str = go_node_info.get('node_id', '')
                if not isinstance(node_id_str, str) or not node_id_str.startswith('S') or 'N' not in node_id_str:
                    logger.error(f"âŒ [ERROR] Goæ¥å£èŠ‚ç‚¹IDæ ¼å¼é”™è¯¯: {node_id_str}")
                    raise ValueError(f"Goæ¥å£èŠ‚ç‚¹IDæ ¼å¼é”™è¯¯: {node_id_str}")
                
                # è§£æS{ShardID}N{NodeID}æ ¼å¼ï¼Œæå–çœŸå®çš„åˆ†ç‰‡ä¿¡æ¯
                try:
                    parts = node_id_str.split('N')
                    if len(parts) != 2:
                        raise ValueError(f"NodeIDæ ¼å¼é”™è¯¯: {node_id_str}")
                    
                    shard_id = int(parts[0][1:])  # å»æ‰'S'å‰ç¼€
                    node_id = int(parts[1])
                    
                    logger.debug(f" [GO_INTERFACE] è§£æèŠ‚ç‚¹: {node_id_str} -> ShardID={shard_id}, NodeID={node_id}")
                    
                except Exception as e:
                    logger.error(f"âŒ [ERROR] è§£æèŠ‚ç‚¹IDå¤±è´¥: {node_id_str}, é”™è¯¯: {e}")
                    raise ValueError(f"è§£æèŠ‚ç‚¹IDå¤±è´¥: {node_id_str}")
                
                # è·å–metadataä¸­çš„åˆ†ç‰‡ä¿¡æ¯ä½œä¸ºéªŒè¯
                metadata = go_node_info.get('metadata', {})
                metadata_shard_id = metadata.get('shard_id')
                if metadata_shard_id is not None and metadata_shard_id != shard_id:
                    logger.warning(f"âš ï¸  [GO_INTERFACE] åˆ†ç‰‡IDä¸ä¸€è‡´: NodeIDä¸­çš„{shard_id} vs metadataä¸­çš„{metadata_shard_id}")
                
                # åˆ›å»ºNodeå¯¹è±¡ï¼Œä½¿ç”¨çœŸå®çš„åˆ†ç‰‡åˆ†é…
                try:
                    from partition.feature.nodeInitialize import Node
                except ImportError:
                    try:
                        from nodeInitialize import Node
                    except ImportError:
                        # åˆ›å»ºåŸºæœ¬çš„Nodeæ›¿ä»£å“
                        class Node:
                            def __init__(self):
                                self.NodeID = 0
                                self.ShardID = 0
                
                real_node = Node()
                real_node.NodeID = node_id
                real_node.ShardID = shard_id  # ä½¿ç”¨ä»NodeIDè§£æå‡ºçš„çœŸå®åˆ†ç‰‡ID
                
                #  [IMPORTANT] è®¾ç½® NodeType - ä»è¾“å…¥æ•°æ®ä¸­æå–
                node_state = go_node_info.get('NodeState', {})
                static_data = node_state.get('Static', {})
                heterogeneous_type = static_data.get('HeterogeneousType', {})
                node_type = heterogeneous_type.get('NodeType', 'full_node')  # é»˜è®¤ä¸ºfull_node
                
                # ç¡®ä¿ Node å¯¹è±¡æœ‰ HeterogeneousType å±æ€§
                if not hasattr(real_node, 'HeterogeneousType'):
                    from partition.feature.nodeInitialize import HeterogeneousTypeLayer
                    real_node.HeterogeneousType = HeterogeneousTypeLayer()
                
                real_node.HeterogeneousType.NodeType = node_type
                
                logger.info(f" [GO_INTERFACE] æˆåŠŸè½¬æ¢èŠ‚ç‚¹: {node_id_str} -> ShardID={shard_id}, NodeID={node_id}, NodeType={node_type}")
                return real_node
            
            def _convert_dict_node_to_real_format(self, dict_node):
                """ğŸš« å·²ç¦ç”¨ï¼šä¸å†æ¥å—ç¼ºå°‘ShardIDçš„å­—å…¸èŠ‚ç‚¹"""
                logger.error(f"âŒ [ERROR] _convert_dict_node_to_real_formatè¢«è°ƒç”¨ï¼Œè¿™è¡¨æ˜ä¼ å…¥äº†å­—å…¸æ ¼å¼çš„èŠ‚ç‚¹")
                logger.error(f"âŒ [ERROR] å­—å…¸èŠ‚ç‚¹å†…å®¹: {dict_node}")
                logger.error(f"âŒ [ERROR] ç³»ç»Ÿéœ€è¦ä»BlockEmulatorè·å–çœŸå®çš„Nodeå¯¹è±¡ï¼Œä¸èƒ½ä½¿ç”¨å­—å…¸æ ¼å¼")
                raise ValueError("ä¸èƒ½è½¬æ¢å­—å…¸æ ¼å¼èŠ‚ç‚¹ä¸ºNodeå¯¹è±¡ï¼Œéœ€è¦çœŸå®çš„BlockEmulator Nodeå¯¹è±¡")
            
            def _create_basic_node(self, node_id=0):
                """ğŸš« å·²ç¦ç”¨ï¼šä¸å†åˆ›å»ºå…·æœ‰å›ºå®šåˆ†ç‰‡åˆ†é…çš„åŸºæœ¬æµ‹è¯•èŠ‚ç‚¹"""
                logger.error(f"âŒ [ERROR] _create_basic_nodeè¢«è°ƒç”¨ï¼Œè¿™è¡¨æ˜æ²¡æœ‰çœŸå®çš„èŠ‚ç‚¹æ•°æ®")
                logger.error(f"âŒ [ERROR] è¯·æ±‚åˆ›å»ºnode_id={node_id}çš„æµ‹è¯•èŠ‚ç‚¹")
                logger.error(f"âŒ [ERROR] ç³»ç»Ÿéœ€è¦ä»BlockEmulatorè·å–çœŸå®çš„èŠ‚ç‚¹æ•°æ®ï¼Œä¸èƒ½ä½¿ç”¨æµ‹è¯•èŠ‚ç‚¹")
                raise ValueError("ä¸èƒ½åˆ›å»ºå…·æœ‰å›ºå®šåˆ†ç‰‡åˆ†é…çš„æµ‹è¯•èŠ‚ç‚¹ï¼Œéœ€è¦çœŸå®çš„BlockEmulatorèŠ‚ç‚¹æ•°æ®")
                    
            def _create_basic_test_data(self):
                """ï¿½ å·²ç¦ç”¨ï¼šä¸å†åˆ›å»ºå…·æœ‰å›ºå®šåˆ†ç‰‡åˆ†é…çš„æµ‹è¯•æ•°æ®"""
                logger.error(f"âŒ [ERROR] _create_basic_test_dataè¢«è°ƒç”¨ï¼Œè¿™è¡¨æ˜æ²¡æœ‰çœŸå®çš„èŠ‚ç‚¹æ•°æ®")
                logger.error(f"âŒ [ERROR] ç³»ç»Ÿéœ€è¦ä»BlockEmulatorè·å–çœŸå®çš„èŠ‚ç‚¹æ•°æ®ï¼Œä¸èƒ½ä½¿ç”¨æµ‹è¯•æ•°æ®")
                raise ValueError("ä¸èƒ½åˆ›å»ºå…·æœ‰å›ºå®šåˆ†ç‰‡åˆ†é…çš„æµ‹è¯•æ•°æ®ï¼Œéœ€è¦çœŸå®çš„BlockEmulatorèŠ‚ç‚¹æ•°æ®")
            
            def _extract_using_real_extractor(self, processed_nodes):
                """
                âš™ï¸ [STEP1] ä½¿ç”¨çœŸå®ç‰¹å¾æå–å™¨å¤„ç†BlockEmulatoræ•°æ®
                
                Args:
                    processed_nodes: BlockEmulatoræä¾›çš„èŠ‚ç‚¹æ•°æ®
                    
                Returns:
                    dict: åŒ…å«f_classicçš„ç‰¹å¾å­—å…¸ï¼Œä¼˜åŒ–çš„80ç»´è¾“å‡º
                """
                try:
                    logger.info(" [STEP1] ä½¿ç”¨ComprehensiveFeatureExtractoræå–ç‰¹å¾")
                    logger.info(f" [STEP1] è¾“å…¥èŠ‚ç‚¹æ•°é‡: {len(processed_nodes)}")
                    
                    #  [DATA_VALIDATION] éªŒè¯è¾“å…¥æ•°æ®
                    if not processed_nodes:
                        logger.error("âŒ [STEP1] è¾“å…¥èŠ‚ç‚¹åˆ—è¡¨ä¸ºç©º")
                        return None
                        
                    first_node = processed_nodes[0]
                    logger.info(f" [STEP1] ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ç±»å‹: {type(first_node)}")
                    if hasattr(first_node, 'NodeID'):
                        logger.info(f" [STEP1] ç¬¬ä¸€ä¸ªèŠ‚ç‚¹NodeID: {first_node.NodeID}")
                    
                    #  [FEATURE_EXTRACTION] è°ƒç”¨çœŸå®çš„ç‰¹å¾æå–å™¨
                    logger.info(f" [STEP1] ç‰¹å¾æå–å™¨ç±»å‹: {type(self.extractor)}")
                    feature_result = self.extractor.extract_features(processed_nodes)
                    
                    if feature_result is None:
                        logger.error("âŒ [STEP1] ç‰¹å¾æå–å™¨è¿”å›None")
                        return None
                    
                    logger.info(f" [STEP1] ç‰¹å¾æå–å™¨è¿”å›ç±»å‹: {type(feature_result)}")
                    
                    # ğŸ“ˆ [DATA_PROCESSING] å¤„ç†è¿”å›çš„ç‰¹å¾æ•°æ®
                    base_features = None
                    
                    if isinstance(feature_result, dict):
                        logger.info(f" [STEP1] å­—å…¸æ ¼å¼ç»“æœï¼Œé”®: {list(feature_result.keys())}")
                        
                        # ComprehensiveFeatureExtractorè¿”å›æ ¼å¼ï¼š{'f_classic': tensor, 'f_graph': tensor, 'nodes': nodes}
                        if 'f_classic' in feature_result and 'f_graph' in feature_result:
                            f_classic_raw = feature_result['f_classic'].to(self.parent.device)
                            f_graph_raw = feature_result['f_graph'].to(self.parent.device)
                            
                            logger.info(f" [STEP1] åŸå§‹f_classicå½¢çŠ¶: {f_classic_raw.shape}")
                            logger.info(f" [STEP1] åŸå§‹f_graphå½¢çŠ¶: {f_graph_raw.shape}")
                            
                            #  [DIMENSION_ALIGNMENT] æå–40ç»´åŸºç¡€ç‰¹å¾
                            if f_classic_raw.shape[1] >= 25 and f_graph_raw.shape[1] >= 15:
                                f_classic_40 = f_classic_raw[:, :25]  # å–å‰25ç»´
                                f_graph_15 = f_graph_raw[:, :15]      # å–å‰15ç»´
                                base_features = torch.cat([f_classic_40, f_graph_15], dim=1)  # [N, 40]
                            else:
                                logger.warning(f"âš ï¸ [STEP1] ç»´åº¦ä¸è¶³ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨40ç»´")
                                if f_classic_raw.shape[1] == 40:
                                    base_features = f_classic_raw
                                else:
                                    # æˆªå–æˆ–å¡«å……åˆ°40ç»´
                                    base_features = self._ensure_40_dimensions(f_classic_raw)
                                    
                        elif 'features' in feature_result:
                            base_features = feature_result['features'].to(self.parent.device)
                        else:
                            logger.error(f"âŒ [STEP1] å­—å…¸ä¸­æœªæ‰¾åˆ°æœŸæœ›çš„ç‰¹å¾é”®: {list(feature_result.keys())}")
                            return None
                            
                    elif hasattr(feature_result, 'to'):
                        logger.info(" [STEP1] å¼ é‡æ ¼å¼ç»“æœ")
                        base_features = feature_result.to(self.parent.device)
                    else:
                        logger.error(f"âŒ [STEP1] ä¸æ”¯æŒçš„è¿”å›æ ¼å¼: {type(feature_result)}")
                        return None
                    
                    if base_features is None:
                        logger.error("âŒ [STEP1] æ— æ³•æå–åŸºç¡€ç‰¹å¾")
                        return None
                    
                    # ğŸ¯ [DIMENSION_OPTIMIZATION] ç¡®ä¿åŸºç¡€ç‰¹å¾ä¸º40ç»´
                    base_features = self._ensure_40_dimensions(base_features)
                    logger.info(f" [STEP1] åŸºç¡€ç‰¹å¾å½¢çŠ¶: {base_features.shape}")
                    
                    # ğŸ“ˆ [FEATURE_PROJECTION] 40ç»´ â†’ 80ç»´æ™ºèƒ½æŠ•å½±
                    f_classic = self._project_to_f_classic(base_features)
                    
                    logger.info(f" [STEP1] f_classicå½¢çŠ¶: {f_classic.shape}")
                    logger.info(f" [STEP1] f_classicèŒƒå›´: [{f_classic.min().item():.3f}, {f_classic.max().item():.3f}]")
                    
                    # ğŸ¯ [FEATURE_DECOMPOSITION] 40ç»´ â†’ 6ç±»ç‰¹å¾å­—å…¸ (Step4ç”¨)
                    six_feature_dict = self._split_40d_to_six_categories(base_features)
                    
                    # ğŸŒ [EDGE_EXTRACTION] ä»Pipelineä¸­è·å–çœŸå®è¾¹ç´¢å¼•ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨HeterogeneousGraphBuilder
                    edge_index = self._extract_edge_index_from_pipeline(feature_result, processed_nodes)
                    
                    # ğŸ—ï¸ [DUAL_FORMAT_OUTPUT] æ„å»ºåŒæ ¼å¼è¾“å‡º
                    result = {
                        # === Step2éœ€è¦çš„MainPipelineå…¼å®¹æ ¼å¼ ===
                        'f_classic': f_classic,    # Step2ç›´æ¥ä½¿ç”¨
                        'f_graph': feature_result.get('f_graph'),  # å¦‚æœæœ‰
                        'f_fused': feature_result.get('f_fused'),  # å¦‚æœæœ‰
                        'nodes': feature_result.get('nodes'),     # èŠ‚ç‚¹ä¿¡æ¯
                        'contrastive_loss': feature_result.get('contrastive_loss', 0.0),
                        
                        # === Step4éœ€è¦çš„6ç±»ç‰¹å¾å­—å…¸ ===
                        'features_dict': {
                            'hardware': six_feature_dict['hardware'],
                            'onchain_behavior': six_feature_dict['onchain_behavior'], 
                            'network_topology': six_feature_dict['network_topology'],
                            'dynamic_attributes': six_feature_dict['dynamic_attributes'],
                            'heterogeneous_type': six_feature_dict['heterogeneous_type'],
                            'categorical': six_feature_dict['categorical']
                        },
                        
                        # === ç³»ç»Ÿéœ€è¦çš„å…ƒæ•°æ® ===
                        'edge_index': edge_index,
                        'num_nodes': len(processed_nodes),
                        'source': 'real_mainpipeline_dual_format',
                        'algorithm': 'MainPipeline_Six_Categories_Plus_F_Classic',
                        'success': True,
                        'metadata': {
                            'use_real_data': True,
                            'extractor_type': 'mainpipeline_dual_output',
                            'f_classic_dim': f_classic.shape[1],
                            'features_dict_dims': {k: v.shape[1] for k, v in six_feature_dict.items()},
                            'base_dimensions': 40,
                            'f_classic_dimensions': self.f_classic_dim,
                            'dual_format': True,
                            'step2_ready': True,
                            'step4_ready': True,
                            'timestamp': datetime.now().isoformat()
                        }
                    }
                    
                    logger.info(f" [STEP1] åŒæ ¼å¼ç‰¹å¾æå–æˆåŠŸ")
                    logger.info(f"   Step2æ ¼å¼: f_classic[{f_classic.shape[1]}ç»´]")
                    logger.info(f"   Step4æ ¼å¼: 6ç±»ç‰¹å¾{[(k, v.shape[1]) for k, v in six_feature_dict.items()]}")
                    logger.info(f"   æ€»è®¡: {sum(v.shape[1] for v in six_feature_dict.values())}ç»´åˆ†è§£ç‰¹å¾ + {f_classic.shape[1]}ç»´æŠ•å½±ç‰¹å¾")
                    return result
                    
                except Exception as e:
                    import traceback
                    logger.error(f"âŒ [STEP1] çœŸå®ç‰¹å¾æå–å¤±è´¥: {str(e)}")
                    logger.error(f"âŒ [STEP1] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                    return None
                try:
                    logger.info(" [DEBUG] ä½¿ç”¨ComprehensiveFeatureExtractoræå–ç‰¹å¾")
                    logger.info(f"ï¿½ [DEBUG] è¾“å…¥èŠ‚ç‚¹æ•°é‡: {len(processed_nodes)}")
                    logger.info(f"ï¿½ [DEBUG] ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ç±»å‹: {type(processed_nodes[0]) if processed_nodes else 'None'}")
                    
                    # æ£€æŸ¥ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„å±æ€§
                    if processed_nodes:
                        first_node = processed_nodes[0]
                        logger.info(f" [DEBUG] ç¬¬ä¸€ä¸ªèŠ‚ç‚¹å±æ€§: {dir(first_node) if hasattr(first_node, '__dict__') else str(first_node)}")
                        if hasattr(first_node, 'NodeID'):
                            logger.info(f" [DEBUG] ç¬¬ä¸€ä¸ªèŠ‚ç‚¹NodeID: {first_node.NodeID}")
                        if hasattr(first_node, 'ShardID'):
                            logger.info(f" [DEBUG] ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ShardID: {first_node.ShardID}")
                    
                    logger.info(f" [DEBUG] ç‰¹å¾æå–å™¨ç±»å‹: {type(self.extractor)}")
                    logger.info(f" [DEBUG] ç‰¹å¾æå–å™¨æ–¹æ³•: {hasattr(self.extractor, 'extract_features')}")
                    
                    # è°ƒç”¨çœŸå®çš„ç‰¹å¾æå–å™¨
                    feature_result = self.extractor.extract_features(processed_nodes)
                    
                    logger.info(f"ï¿½ [DEBUG] ç‰¹å¾æå–å™¨è¿”å›ç±»å‹: {type(feature_result)}")
                    logger.info(f" [DEBUG] ç‰¹å¾æå–å™¨è¿”å›å€¼æ¦‚è§ˆ: {str(feature_result)[:200]}...")
                    
                    # æ£€æŸ¥è¿”å›ç»“æœç±»å‹
                    if feature_result is None:
                        logger.error("âŒ [CRITICAL] ç‰¹å¾æå–å™¨è¿”å›None")
                        return None
                    elif isinstance(feature_result, dict):
                        logger.info(" [FORMAT] ç‰¹å¾æå–å™¨è¿”å›å­—å…¸æ ¼å¼")
                        logger.info(f" [FORMAT] å­—å…¸é”®: {list(feature_result.keys())}")
                        
                        # è¯¦ç»†æ£€æŸ¥æ¯ä¸ªé”®çš„å†…å®¹
                        for key, value in feature_result.items():
                            if isinstance(value, torch.Tensor):
                                logger.info(f" [DEBUG] {key}: {value.shape}, dtype={value.dtype}, device={value.device}")
                            else:
                                logger.info(f" [DEBUG] {key}: {type(value)}, {str(value)[:100]}...")
                        
                        # ComprehensiveFeatureExtractorè¿”å›å­—å…¸æ ¼å¼ï¼š{'f_classic': tensor, 'f_graph': tensor, 'nodes': nodes}
                        if 'f_classic' in feature_result and 'f_graph' in feature_result:
                            f_classic = feature_result['f_classic'].to(self.parent.device)
                            f_graph = feature_result['f_graph'].to(self.parent.device)
                            
                            logger.info(f" [TENSOR] F_classicå½¢çŠ¶: {f_classic.shape}")
                            logger.info(f" [TENSOR] F_graphå½¢çŠ¶: {f_graph.shape}")
                            
                            # åˆå¹¶F_classicå’ŒF_graphä¸º40ç»´ç‰¹å¾
                            # F_classic: [N, 128] -> å–å‰25ç»´
                            # F_graph: [N, 96] -> å–å‰15ç»´  
                            # æ€»è®¡40ç»´ (25 + 15 = 40)
                            f_classic_40 = f_classic[:, :25]  # å–å‰25ç»´
                            f_graph_15 = f_graph[:, :15]      # å–å‰15ç»´
                            
                            feature_tensor = torch.cat([f_classic_40, f_graph_15], dim=1)  # [N, 40]
                            
                            logger.info(f" [TENSOR] åˆå¹¶åç‰¹å¾ç»´åº¦: {feature_tensor.shape}")
                            
                        elif 'features' in feature_result:
                            feature_tensor = feature_result['features'].to(self.parent.device)
                            logger.info(f" [TENSOR] å•ä¸€ç‰¹å¾å¼ é‡: {feature_tensor.shape}")
                        else:
                            logger.error("âŒ [FORMAT] å­—å…¸æ ¼å¼ç»“æœä¸­æœªæ‰¾åˆ°æœŸæœ›çš„ç‰¹å¾é”®")
                            logger.error(f"âŒ [FORMAT] å¯ç”¨é”®: {list(feature_result.keys())}")
                            return None
                            
                    elif hasattr(feature_result, 'to'):
                        logger.info(" [FORMAT] ç‰¹å¾æå–å™¨è¿”å›å¼ é‡æ ¼å¼")
                        feature_tensor = feature_result.to(self.parent.device)
                        logger.info(f" [TENSOR] ç›´æ¥å¼ é‡å½¢çŠ¶: {feature_tensor.shape}")
                    else:
                        logger.error(f"âŒ [EXTRACTOR] ç‰¹å¾æå–å™¨è¿”å›äº†ä¸æ”¯æŒçš„æ ¼å¼: {type(feature_result)}")
                        return None
                    
                    logger.info(f" [EXTRACTOR] çœŸå®ç‰¹å¾æå–å®Œæˆï¼Œæœ€ç»ˆç»´åº¦: {feature_tensor.shape}")
                    logger.info(f" [EXTRACTOR] ç‰¹å¾èŒƒå›´: [{feature_tensor.min().item():.3f}, {feature_tensor.max().item():.3f}]")
                    
                    # éªŒè¯ç»´åº¦æ˜¯å¦æ­£ç¡®
                    expected_dim = sum(self.feature_dims.values())
                    if feature_tensor.shape[1] != expected_dim:
                        logger.warning(f"âš ï¸ [EXTRACTOR] ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼šæœŸæœ›{expected_dim}ï¼Œå®é™…{feature_tensor.shape[1]}")
                        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œè°ƒæ•´
                        if feature_tensor.shape[1] > expected_dim:
                            feature_tensor = feature_tensor[:, :expected_dim]
                            logger.info(f"âœ‚ï¸ [EXTRACTOR] æˆªå–åˆ°æœŸæœ›ç»´åº¦: {feature_tensor.shape}")
                        else:
                            # å¦‚æœç»´åº¦ä¸è¶³ï¼Œç”¨é›¶å¡«å……
                            padding = torch.zeros(feature_tensor.shape[0], expected_dim - feature_tensor.shape[1], 
                                                device=feature_tensor.device)
                            feature_tensor = torch.cat([feature_tensor, padding], dim=1)
                            logger.info(f"ğŸ“¦ [EXTRACTOR] å¡«å……åˆ°æœŸæœ›ç»´åº¦: {feature_tensor.shape}")
                    
                    # å°†40ç»´ç‰¹å¾åˆ†å‰²ä¸º5ç±»
                    features_dict = self._split_features_to_categories(feature_tensor)
                    
                    return features_dict
                    
                except Exception as e:
                    logger.error(f"âŒ [EXTRACTOR] çœŸå®ç‰¹å¾æå–å™¨è°ƒç”¨å¤±è´¥: {e}")
                    logger.error("âŒ [EXTRACTOR] è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
                    import traceback
                    traceback.print_exc()
                    return None

            def _ensure_40_dimensions(self, feature_tensor):
                """
                 [DIMENSION_ALIGNMENT] ç¡®ä¿ç‰¹å¾å¼ é‡ä¸º40ç»´
                
                Args:
                    feature_tensor: è¾“å…¥ç‰¹å¾å¼ é‡ [N, D]
                    
                Returns:
                    torch.Tensor: 40ç»´ç‰¹å¾å¼ é‡ [N, 40]
                """
                current_dim = feature_tensor.shape[1]
                target_dim = 40
                
                if current_dim == target_dim:
                    return feature_tensor
                elif current_dim > target_dim:
                    # æˆªå–å‰40ç»´
                    logger.info(f"âœ‚ï¸ [DIMENSION_ALIGNMENT] æˆªå– {current_dim}ç»´ â†’ {target_dim}ç»´")
                    return feature_tensor[:, :target_dim]
                else:
                    # é›¶å¡«å……åˆ°40ç»´
                    padding_dim = target_dim - current_dim
                    padding = torch.zeros(feature_tensor.shape[0], padding_dim, device=feature_tensor.device)
                    logger.info(f"ğŸ“¦ [DIMENSION_ALIGNMENT] å¡«å…… {current_dim}ç»´ â†’ {target_dim}ç»´")
                    return torch.cat([feature_tensor, padding], dim=1)
            
            def _project_to_f_classic(self, base_features):
                """
                ğŸ“ˆ [FEATURE_PROJECTION] å°†40ç»´åŸºç¡€ç‰¹å¾æŠ•å½±åˆ°80ç»´f_classic
                
                Args:
                    base_features: 40ç»´åŸºç¡€ç‰¹å¾ [N, 40]
                    
                Returns:
                    torch.Tensor: 80ç»´f_classicç‰¹å¾ [N, 80]
                """
                # åˆå§‹åŒ–æŠ•å½±å±‚ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
                if not hasattr(self, 'feature_projector'):
                    self.feature_projector = torch.nn.Sequential(
                        torch.nn.Linear(40, 64),
                        torch.nn.ReLU(),
                        torch.nn.Linear(64, self.f_classic_dim),
                        torch.nn.Tanh()  # å½’ä¸€åŒ–è¾“å‡º
                    ).to(self.parent.device)
                    
                    logger.info(f" [FEATURE_PROJECTION] åˆå§‹åŒ–æŠ•å½±å™¨: 40 â†’ 64 â†’ {self.f_classic_dim}")
                
                # æ‰§è¡Œç‰¹å¾æŠ•å½±
                with torch.no_grad():  # ä¸éœ€è¦æ¢¯åº¦
                    f_classic = self.feature_projector(base_features)
                
                logger.debug(f"ğŸ“ˆ [FEATURE_PROJECTION] æŠ•å½±å®Œæˆ: {base_features.shape} â†’ {f_classic.shape}")
                return f_classic

            def _split_40d_to_six_categories(self, base_features):
                """
                 [FEATURE_DECOMPOSITION] å°†40ç»´åŸºç¡€ç‰¹å¾åˆ†è§£ä¸º6ç±»ç‰¹å¾å­—å…¸
                
                Args:
                    base_features: 40ç»´åŸºç¡€ç‰¹å¾å¼ é‡ [N, 40]
                    
                Returns:
                    dict: 6ç±»ç‰¹å¾å­—å…¸
                """
                num_nodes = base_features.shape[0]
                device = base_features.device
                
                # åŸºäºçœŸå®40ç»´ç‰¹å¾ç»“æ„è¿›è¡Œåˆ†è§£
                # hardware: 11ç»´, network_topology: 5ç»´, heterogeneous_type: 2ç»´
                # onchain_behavior: 15ç»´, dynamic_attributes: 7ç»´
                # æ€»è®¡: 11+5+2+15+7 = 40ç»´
                
                six_categories = {
                    # ç¡¬ä»¶ç‰¹å¾ (11ç»´) - CPU(2) + Memory(3) + Storage(3) + Network(3)
                    'hardware': base_features[:, :11].clone(),
                    
                    # ç½‘ç»œæ‹“æ‰‘ç‰¹å¾ (5ç»´) - è¿æ¥æ•°ã€å±‚æ¬¡ã€ä¸­å¿ƒæ€§ç­‰
                    'network_topology': base_features[:, 11:16].clone(),
                    
                    # å¼‚æ„ç±»å‹ç‰¹å¾ (2ç»´) - èŠ‚ç‚¹ç±»å‹ã€æ ¸å¿ƒèµ„æ ¼
                    'heterogeneous_type': base_features[:, 16:18].clone(),
                    
                    # é“¾ä¸Šè¡Œä¸ºç‰¹å¾ (15ç»´) - äº¤æ˜“èƒ½åŠ›ã€è·¨åˆ†ç‰‡ã€åŒºå—ç”Ÿæˆç­‰
                    'onchain_behavior': base_features[:, 18:33].clone(),
                    
                    # åŠ¨æ€å±æ€§ç‰¹å¾ (7ç»´) - CPUä½¿ç”¨ç‡ã€å†…å­˜ä½¿ç”¨ç‡ç­‰
                    'dynamic_attributes': base_features[:, 33:40].clone(),
                    
                    # åˆ†ç±»ç‰¹å¾ (é¢å¤–ç”Ÿæˆ) - ä¸ºäº†å…¼å®¹ç»Ÿä¸€åé¦ˆå¼•æ“å¯èƒ½éœ€è¦çš„é¢å¤–åˆ†ç±»ä¿¡æ¯
                    'categorical': torch.randn(num_nodes, 8, device=device) * 0.1  # å°å¹…éšæœºåˆ†ç±»ç‰¹å¾
                }
                
                # éªŒè¯ç»´åº¦
                total_dims = sum(v.shape[1] for v in six_categories.values())
                logger.debug(f" [FEATURE_DECOMPOSITION] 40ç»´åˆ†è§£å®Œæˆ:")
                for category, tensor in six_categories.items():
                    logger.debug(f"   {category}: {tensor.shape[1]}ç»´")
                logger.debug(f"   æ€»è®¡: {total_dims}ç»´ (40ç»´åŸºç¡€ + 8ç»´åˆ†ç±»)")
                
                return six_categories

            def _split_features_to_categories(self, feature_tensor):
                """å°†40ç»´ç‰¹å¾åˆ†å‰²ä¸º5ä¸ªç±»åˆ«"""
                features_dict = {}
                start_idx = 0
                
                for category, dim in self.feature_dims.items():
                    end_idx = start_idx + dim
                    features_dict[category] = feature_tensor[:, start_idx:end_idx]
                    start_idx = end_idx
                    
                    logger.info(f"ç‰¹å¾ç±»åˆ« {category}: {features_dict[category].shape}")
                
                return features_dict

            def _extract_edge_index_from_pipeline(self, feature_result, processed_nodes=None):
                """ä»Pipelineç‰¹å¾æå–å™¨ä¸­è·å–è¾¹ç´¢å¼•ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨çœŸå®çš„HeterogeneousGraphBuilder"""
                try:
                    # ä¼˜å…ˆä»feature_resultä¸­è·å–è¾¹ç´¢å¼•
                    if isinstance(feature_result, dict):
                        if 'edge_index' in feature_result:
                            edge_index = feature_result['edge_index']
                            if edge_index.size(1) > 0:  # æ£€æŸ¥è¾¹ç´¢å¼•ä¸ä¸ºç©º
                                logger.info(f" [EDGE_EXTRACTION] ä»feature_resultè·å–è¾¹ç´¢å¼•: {edge_index.shape}")
                                return edge_index
                    
                    # å°è¯•ä»ç‰¹å¾æå–å™¨è·å–è¾¹ç´¢å¼•
                    if hasattr(self.extractor, 'get_last_edge_index'):
                        edge_index = self.extractor.get_last_edge_index()
                        if edge_index is not None and edge_index.size(1) > 0:
                            logger.info(f" [EDGE_EXTRACTION] ä»extractorè·å–è¾¹ç´¢å¼•: {edge_index.shape}")
                            return edge_index
                    
                    # å°è¯•ä»GraphFeatureExtractorè·å–
                    if hasattr(self.extractor, 'graph_extractor'):
                        if hasattr(self.extractor.graph_extractor, 'get_adjacency_info'):
                            adjacency_info = self.extractor.graph_extractor.get_adjacency_info()
                            if 'edge_index' in adjacency_info:
                                edge_index = adjacency_info['edge_index']
                                if edge_index.size(1) > 0:
                                    logger.info(f" [EDGE_EXTRACTION] ä»GraphFeatureExtractorè·å–è¾¹ç´¢å¼•: {edge_index.shape}")
                                    return edge_index
                    
                    # å¦‚æœPipelineæ— æ³•æä¾›è¾¹ç´¢å¼•ï¼Œä½¿ç”¨çœŸå®çš„HeterogeneousGraphBuilderç”Ÿæˆ
                    logger.info(" [EDGE_EXTRACTION] Pipelineæœªæä¾›æœ‰æ•ˆè¾¹ç´¢å¼•ï¼Œä½¿ç”¨HeterogeneousGraphBuilderç”ŸæˆçœŸå®è¾¹ç´¢å¼•")
                    if processed_nodes is not None:
                        return self._generate_realistic_edge_index(processed_nodes)
                    else:
                        logger.warning("âš ï¸  [EDGE_EXTRACTION] æ— èŠ‚ç‚¹æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆè¾¹ç´¢å¼•")
                        return torch.empty((2, 0), dtype=torch.long, device=self.parent.device)
                    
                except Exception as e:
                    logger.error(f"âŒ [EDGE_EXTRACTION] æå–è¾¹ç´¢å¼•å¤±è´¥: {e}")
                    # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨èŠ‚ç‚¹æ•°æ®ç”Ÿæˆè¾¹ç´¢å¼•
                    if processed_nodes is not None:
                        try:
                            return self._generate_realistic_edge_index(processed_nodes)
                        except Exception as e2:
                            logger.error(f"âŒ [EDGE_EXTRACTION] HeterogeneousGraphBuilderä¹Ÿå¤±è´¥: {e2}")
                    return torch.empty((2, 0), dtype=torch.long, device=self.parent.device)

            
            def _generate_realistic_edge_index(self, processed_nodes):
                """ä½¿ç”¨å¼‚æ„å›¾æ„å»ºå™¨ç”ŸæˆçœŸå®çš„è¾¹ç´¢å¼•"""
                if self.parent.heterogeneous_graph_builder is None:
                    raise RuntimeError("HeterogeneousGraphBuilder æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ„å»ºæ­£ç¡®çš„å¼‚æ„å›¾")
                
                try:
                    # ç¡®ä¿processed_nodesæ˜¯Nodeå¯¹è±¡åˆ—è¡¨
                    if not processed_nodes:
                        logger.error("æ²¡æœ‰èŠ‚ç‚¹æ•°æ®ï¼Œæ— æ³•æ„å»ºå›¾")
                        raise ValueError("èŠ‚ç‚¹åˆ—è¡¨ä¸ºç©º")
                    
                    # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æœ‰HeterogeneousTypeå±æ€§
                    valid_nodes = []
                    node_types = []
                    for i, node in enumerate(processed_nodes):
                        if hasattr(node, 'HeterogeneousType') and hasattr(node.HeterogeneousType, 'NodeType'):
                            valid_nodes.append(node)
                            node_type = getattr(node.HeterogeneousType, 'NodeType', 'unknown')
                            node_types.append(node_type)
                            logger.info(f"èŠ‚ç‚¹ {i}: ç±»å‹ {node_type}")
                        else:
                            logger.warning(f"èŠ‚ç‚¹ {getattr(node, 'NodeID', 'unknown')} ç¼ºå°‘å¼‚æ„ç±»å‹ä¿¡æ¯")
                    
                    if not valid_nodes:
                        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„å¼‚æ„èŠ‚ç‚¹æ•°æ®")
                        raise ValueError("æ‰€æœ‰èŠ‚ç‚¹éƒ½ç¼ºå°‘å¼‚æ„ç±»å‹ä¿¡æ¯")
                    
                    # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
                    from collections import Counter
                    type_counts = Counter(node_types)
                    logger.info(f"èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {dict(type_counts)}")
                    
                    # ä½¿ç”¨å¼‚æ„å›¾æ„å»ºå™¨æ„å»ºå›¾
                    edge_index, edge_type = self.parent.heterogeneous_graph_builder.build_graph(valid_nodes)
                    
                    logger.info(f"æˆåŠŸæ„å»ºå¼‚æ„å›¾: {edge_index.size(1)} æ¡è¾¹, {len(valid_nodes)} ä¸ªèŠ‚ç‚¹")
                    logger.info(f"è¾¹ç±»å‹åˆ†å¸ƒ: {torch.bincount(edge_type) if edge_type.numel() > 0 else 'æ— è¾¹'}")
                    
                    return edge_index
                    
                except Exception as e:
                    logger.error(f"å¼‚æ„å›¾æ„å»ºå¤±è´¥: {e}")
                    raise RuntimeError(f"å¼‚æ„å›¾æ„å»ºå¤±è´¥ï¼Œå¿…é¡»ä½¿ç”¨æ­£ç¡®çš„å®ç°: {e}")
            
            def process_transaction_data(self, tx_data):
                """å¤„ç†äº¤æ˜“æ•°æ®"""
                try:
                    if self.adapter and hasattr(self.adapter, 'extract_features'):
                        # ä½¿ç”¨çœŸå®é€‚é…å™¨æå–ç‰¹å¾
                        features = self.adapter.extract_features(tx_data)
                        logger.info(f"ä½¿ç”¨çœŸå®é€‚é…å™¨æå–ç‰¹å¾: {features.shape if hasattr(features, 'shape') else len(features)}")
                        return features
                    else:
                        logger.error("é€‚é…å™¨æœªæ­£ç¡®åˆå§‹åŒ–")
                        raise RuntimeError("ç‰¹å¾æå–å¤±è´¥ï¼šé€‚é…å™¨æœªæ­£ç¡®åˆå§‹åŒ–")
                        
                except Exception as e:
                    logger.error(f"ç‰¹å¾æå–å¤±è´¥: {e}")
                    # ä¸ä½¿ç”¨ä»»ä½•å¤‡ç”¨ç»“æœ
                    raise RuntimeError(f"ç‰¹å¾æå–å¤±è´¥ï¼Œå¿…é¡»ä½¿ç”¨çœŸå®å®ç°: {e}")
                
        return SimpleStep1Processor(self)
    
    def initialize_step2(self):
        """åˆå§‹åŒ–ç¬¬äºŒæ­¥ï¼šå¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ """
        logger.info("åˆå§‹åŒ–Step2ï¼šå¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ ")
        
        try:
            # ç›´æ¥å¯¼å…¥çœŸå®çš„All_Finalå®ç°
            sys.path.insert(0, str(Path(__file__).parent / "muti_scale"))
            from All_Final import TemporalMSCIA
            
            config = self.config["step2"]
            
            # ğŸ¯ [DIMENSION_ALIGNMENT] ä½¿ç”¨ä¼˜åŒ–çš„f_classicç»´åº¦
            f_classic_dim = self.f_classic_dim  # ä½¿ç”¨è®¡ç®—å¾—å‡ºçš„80ç»´
            
            logger.info(f" [OPTIMIZATION] Step2è¾“å…¥ç»´åº¦é…ç½®:")
            logger.info(f"   åŸå§‹æ•°æ®å­—æ®µ: {sum(self.real_feature_dims.values())}ç»´ (BlockEmulatorçš„40ä¸ªçœŸå®å­—æ®µ)")
            logger.info(f"   f_classicç»´åº¦: {f_classic_dim}ç»´ (ä¼˜åŒ–åçš„æŠ•å½±ç»´åº¦)")
            logger.info(f"   æ•°æ®æµ: Step1.f_classic[{f_classic_dim}] + adjacency_matrix â†’ Step2")
            
            # åˆ›å»ºçœŸå®çš„TemporalMSCIAæ¨¡å‹
            self.step2_processor = TemporalMSCIA(
                input_dim=f_classic_dim,  # ä½¿ç”¨ä¼˜åŒ–çš„f_classicç»´åº¦ (80)
                hidden_dim=config.get("hidden_dim", 64),
                time_dim=config.get("time_dim", 16),
                k_ratio=config.get("k_ratio", 0.9),
                alpha=config.get("alpha", 0.3),
                beta=config.get("beta", 0.4),
                gamma=config.get("gamma", 0.3),
                tau=config.get("tau", 0.09),
                num_node_types=config.get("num_node_types", 5),
                num_edge_types=config.get("num_edge_types", 3)
            ).to(self.device)
            
            logger.info("Step2å¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ å™¨åˆå§‹åŒ–æˆåŠŸ - å·²ä¼˜åŒ–ä¸ºä½¿ç”¨f_classicè¾“å…¥")
            
        except Exception as e:
            logger.error(f"Step2åˆå§‹åŒ–å¤±è´¥: {e}")
            # å¿…é¡»ä½¿ç”¨çœŸå®å®ç°
            raise RuntimeError(f"Step2åˆå§‹åŒ–å¤±è´¥ï¼Œå¿…é¡»ä½¿ç”¨çœŸå®All_Final.pyå®ç°: {e}")
            
        except Exception as e:
            logger.error(f"Step2åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä¸ä½¿ç”¨å¤‡ç”¨å¤„ç†å™¨
            raise RuntimeError(f"Step2åˆå§‹åŒ–å¤±è´¥ï¼Œå¿…é¡»ä½¿ç”¨çœŸå®å¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ : {e}")
    
    def initialize_step3(self):
        """åˆå§‹åŒ–ç¬¬ä¸‰æ­¥ï¼šEvolveGCNåˆ†ç‰‡"""
        logger.info("åˆå§‹åŒ–Step3ï¼šEvolveGCNåˆ†ç‰‡")
        
        try:
            # å¯¼å…¥çœŸå®çš„EvolveGCNæ¨¡å—
            sys.path.append(str(Path(__file__).parent / "evolve_GCN"))
            from models.evolve_gcn_wrapper import EvolveGCNWrapper
            
            config = self.config["step3"]
            
            # Step3æ¥æ”¶Step2çš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œç»´åº¦æ˜¯Step2çš„embed_dim
            step2_output_dim = self.config["step2"]["embed_dim"]  # 64ç»´
            
            # åˆ›å»ºçœŸå®çš„EvolveGCNåŒ…è£…å™¨
            self.step3_processor = EvolveGCNWrapper(
                input_dim=step2_output_dim,  # ä½¿ç”¨Step2è¾“å‡ºç»´åº¦è€ŒéåŸå§‹ç‰¹å¾ç»´åº¦
                hidden_dim=config["hidden_dim"]
            ).to(self.device)  # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            
            logger.info(f"Step3 EvolveGCNåˆ†ç‰‡å™¨åˆå§‹åŒ–æˆåŠŸï¼Œè®¾å¤‡: {self.device}")
            
        except Exception as e:
            logger.error(f"Step3åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä¸ä½¿ç”¨å¤‡ç”¨å¤„ç†å™¨
            raise RuntimeError(f"Step3åˆå§‹åŒ–å¤±è´¥ï¼Œå¿…é¡»ä½¿ç”¨çœŸå®EvolveGCN: {e}")
            

    
    def initialize_step4(self):
        """åˆå§‹åŒ–ç¬¬å››æ­¥ï¼šç»Ÿä¸€åé¦ˆæœºåˆ¶"""
        logger.info("åˆå§‹åŒ–Step4ï¼šç»Ÿä¸€åé¦ˆæœºåˆ¶")
        
        try:
            # å¯¼å…¥çœŸå®çš„ç»Ÿä¸€åé¦ˆå¼•æ“
            from feedback.unified_feedback_engine import UnifiedFeedbackEngine
            
            # ä½¿ç”¨çœŸå®çš„40ç»´ç‰¹å¾é…ç½®
            feature_dims = self.real_feature_dims  # ä½¿ç”¨40ç»´çœŸå®ç‰¹å¾ç»´åº¦
            
            # ç¡®ä¿é…ç½®å®Œæ•´
            step4_config = self.config["step4"]
            logger.info(f"Step4é…ç½®: {step4_config}")
            logger.info(f"çœŸå®ç‰¹å¾ç»´åº¦: {feature_dims}")
            
            self.step4_processor = UnifiedFeedbackEngine(
                feature_dims=feature_dims,
                config=step4_config,
                device=str(self.device)
            )
            
            logger.info("Step4 çœŸå®ç»Ÿä¸€åé¦ˆå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"Step4åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä¸ä½¿ç”¨å¤‡ç”¨å¤„ç†å™¨
            raise RuntimeError(f"Step4åˆå§‹åŒ–å¤±è´¥ï¼Œå¿…é¡»ä½¿ç”¨çœŸå®ç»Ÿä¸€åé¦ˆ: {e}")
            

    
    def initialize_all_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        logger.info("=== åˆå§‹åŒ–æ‰€æœ‰ç³»ç»Ÿç»„ä»¶ ===")
        
        try:
            self.initialize_step1()
            self.initialize_step2()
            self.initialize_step3()
            self.initialize_step4()
            
            logger.info("æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise RuntimeError(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­: {e}")
    
    def run_step1_feature_extraction(self, node_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        è¿è¡Œç¬¬ä¸€æ­¥ï¼šç‰¹å¾æå–
        
        Args:
            node_data: èŠ‚ç‚¹æ•°æ®ï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
        
        Returns:
            åŒ…å«ç‰¹å¾çš„å­—å…¸
        """
        logger.info("æ‰§è¡ŒStep1ï¼šç‰¹å¾æå–")
        
        try:
            # ç¡®ä¿Step1å¤„ç†å™¨å·²åˆå§‹åŒ–
            if self.step1_processor is None:
                logger.info("Step1å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
                self.initialize_step1()
            
            # === Step1è¾“å…¥å‚æ•° ===
            logger.info("=== Step1 ç‰¹å¾æå–å‚æ•° ===")
            if node_data:
                if isinstance(node_data, dict) and 'nodes' in node_data:
                    logger.info(f"   å¤–éƒ¨èŠ‚ç‚¹æ•°æ®: {len(node_data['nodes'])} ä¸ªèŠ‚ç‚¹")
                else:
                    logger.info(f"   å¤–éƒ¨èŠ‚ç‚¹æ•°æ®: {len(node_data)} ä¸ªèŠ‚ç‚¹")
            else:
                logger.info("ğŸ“‹ [TEST_DATA] ä½¿ç”¨æµ‹è¯•èŠ‚ç‚¹æ•°æ®è¿›è¡Œæ¼”ç¤º")
                logger.info("ğŸ“‹ [TEST_DATA] æµ‹è¯•æ•°æ®åŒ…å«40ç»´çœŸå®ç‰¹å¾ç»“æ„ï¼Œä»…ç”¨äºåŠŸèƒ½éªŒè¯")
            logger.info(f"   ç‰¹å¾é…ç½®: {sum(self.real_feature_dims.values())}ç»´ (6ç±»), è®¾å¤‡: {self.device}")
            
            if hasattr(self.step1_processor, 'extract_real_features'):
                # è®°å½•ç‰¹å¾æå–å¼€å§‹æ—¶é—´
                extraction_start = time.time()
                
                # ä½¿ç”¨çœŸå®ç³»ç»Ÿæ¥å£
                result = self.step1_processor.extract_real_features(
                    node_data=node_data,
                    feature_dims=self.real_feature_dims
                )
                
                extraction_time = time.time() - extraction_start
                logger.info(f"   Step1ç‰¹å¾æå–è€—æ—¶: {extraction_time:.3f}ç§’")
            else:
                raise RuntimeError("Step1å¤„ç†å™¨ç¼ºå°‘extract_real_featuresæ–¹æ³•")
            
            # === Step1è¾“å‡ºç»“æœ ===
            logger.info("=== Step1 è¾“å‡ºç»“æœ ===")
            
            # éªŒè¯ç‰¹å¾ç»´åº¦
            self._validate_step1_output(result)
            
            # è®°å½•MainPipelineæ ¼å¼ç‰¹å¾è¯¦æƒ…
            if 'f_classic' in result:
                f_classic = result['f_classic']
                logger.info(f"   F_classic: å½¢çŠ¶{f_classic.shape}, èŒƒå›´[{f_classic.min().item():.2f}, {f_classic.max().item():.2f}]")
                
                # åªåœ¨f_graphå­˜åœ¨ä¸”ä¸ä¸ºNoneæ—¶è®°å½•
                f_graph = result.get('f_graph')
                if f_graph is not None:
                    logger.info(f"   F_graph: å½¢çŠ¶{f_graph.shape}, èŒƒå›´[{f_graph.min().item():.2f}, {f_graph.max().item():.2f}]")
                else:
                    logger.info("   F_graph: None (ä¼˜åŒ–è·³è¿‡)")
                
                # åªåœ¨f_fusedå­˜åœ¨ä¸”ä¸ä¸ºNoneæ—¶è®°å½•
                f_fused = result.get('f_fused')
                if f_fused is not None:
                    logger.info(f"   F_fused: å½¢çŠ¶{f_fused.shape}, èŒƒå›´[{f_fused.min().item():.2f}, {f_fused.max().item():.2f}]")
                else:
                    logger.info("   F_fused: None (ä¼˜åŒ–è·³è¿‡)")
                    
                logger.info(" [OPTIMIZATION] æ•°æ®æµä¼˜åŒ–å®Œæˆï¼šStep2å°†ç›´æ¥ä½¿ç”¨f_classic[80ç»´]")
                
            # å¤‡ç”¨ï¼šè®°å½•æ—§æ ¼å¼ç‰¹å¾è¯¦æƒ…
            elif 'features' in result:
                features = result['features']
                logger.info(f"   ç‰¹å¾ç±»åˆ«ï¼ˆæ—§æ ¼å¼ï¼‰: {list(features.keys())}")
                
                total_feature_dim = 0
                for name, tensor in features.items():
                    total_feature_dim += tensor.shape[1]
                    logger.info(f"   {name}: å½¢çŠ¶{tensor.shape}, èŒƒå›´[{tensor.min().item():.2f}, {tensor.max().item():.2f}]")
                
                logger.info(f"   æ€»ç‰¹å¾ç»´åº¦: {total_feature_dim}")
            
            # è®°å½•è¾¹ç´¢å¼•ä¿¡æ¯
            if 'edge_index' in result:
                edge_index = result['edge_index']
                logger.info(f"   è¾¹ç´¢å¼•: {edge_index.shape}, è¾¹æ•°{edge_index.shape[1]}")
                if edge_index.shape[1] > 0:
                    self_loops = (edge_index[0] == edge_index[1]).sum().item()
                    logger.info(f"   èŠ‚ç‚¹èŒƒå›´: [{edge_index.min().item()}, {edge_index.max().item()}], è‡ªç¯: {self_loops}")
            else:
                logger.warning("   âŒ Step1æœªç”Ÿæˆè¾¹ç´¢å¼•")
            
            # ä¿å­˜ç»“æœ
            step1_file = self.output_dir / "step1_features.pkl"
            with open(step1_file, 'wb') as f:
                pickle.dump(result, f)
            
            logger.info("Step1ç‰¹å¾æå–å®Œæˆ")
            if 'f_classic' in result:
                f_graph = result.get('f_graph')
                if f_graph is not None:
                    logger.info(f"   MainPipelineæ ¼å¼: f_classic{result['f_classic'].shape}, f_graph{f_graph.shape}")
                else:
                    logger.info(f"   MainPipelineæ ¼å¼: f_classic{result['f_classic'].shape}, f_graph=None(ä¼˜åŒ–è·³è¿‡)")
            elif 'features' in result:
                logger.info(f"   ç‰¹å¾ç±»åˆ«: {list(result['features'].keys())}")
            logger.info(f"   èŠ‚ç‚¹æ•°é‡: {result.get('num_nodes', 'Unknown')}")
            logger.info(f"   ç»“æœæ–‡ä»¶: {step1_file}")
            
            return result
            
        except Exception as e:
            logger.error(f"Step1æ‰§è¡Œå¤±è´¥: {e}")
            raise RuntimeError(f"Step1æ‰§è¡Œå¤±è´¥ï¼Œä¸ä½¿ç”¨å¤‡ç”¨å®ç°: {e}")
    
    def extract_features_step1(self, node_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        åˆ«åæ–¹æ³•ï¼šå‘åå…¼å®¹ï¼Œè°ƒç”¨run_step1_feature_extraction
        
        Args:
            node_data: èŠ‚ç‚¹æ•°æ®ï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
        
        Returns:
            åŒ…å«ç‰¹å¾çš„å­—å…¸
        """
        return self.run_step1_feature_extraction(node_data)
    
    def run_step2_multiscale_learning(self, step1_output: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¿è¡Œç¬¬äºŒæ­¥ï¼šå¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ 
        
        Args:
            step1_output: Step1çš„è¾“å‡ºç»“æœ
            
        Returns:
            å¤šå°ºåº¦å­¦ä¹ çš„ç»“æœ
        """
        logger.info("æ‰§è¡ŒStep2ï¼šå¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ ")
        
        try:
            # ä»Step1è·å–MainPipelineçš„è¾“å‡º
            f_classic = step1_output.get('f_classic')
            f_graph = step1_output.get('f_graph')
            edge_index = step1_output.get('edge_index')
            
            # === Step2è¾“å…¥å‚æ•° ===
            logger.info("=== Step2 è¾“å…¥å‚æ•° ===")
            logger.info(f" [DEBUG] Step1è¾“å‡ºé”®: {list(step1_output.keys())}")
            
            if f_classic is not None:
                logger.info(f" [TENSOR] F_classic: {f_classic.shape}, æ•°æ®æµä¼˜åŒ–")
                logger.info(f" [DEBUG] F_classicèŒƒå›´: [{f_classic.min().item():.3f}, {f_classic.max().item():.3f}]")
                logger.info(f" [DEBUG] F_classicè®¾å¤‡: {f_classic.device}")
                
                # ğŸ¯ [OPTIMIZATION] Step2ç›´æ¥ä½¿ç”¨f_classicï¼ˆ80ç»´ï¼‰ä½œä¸ºè¾“å…¥ï¼
                logger.info(f" [OPTIMIZATION] ä½¿ç”¨f_classic[{f_classic.shape[1]}ç»´]ä½œä¸ºStep2è¾“å…¥")
                logger.info(" [OPTIMIZATION] è·³è¿‡40ç»´ç‰¹å¾åˆå¹¶ï¼Œä¿æŒé«˜ç»´è¯­ä¹‰è¡¨ç¤º")
                
                # ç¡®ä¿f_classicåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                input_features = f_classic.to(self.device)
                
            else:
                logger.error("âŒ [FORMAT] Step1æœªæä¾›f_classicï¼Œå°è¯•å¤‡ç”¨å¤„ç†")
                
                # å¤‡ç”¨ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ—§æ ¼å¼çš„featureså­—å…¸
                if 'features' in step1_output:
                    logger.info(" [COMPATIBILITY] æ£€æµ‹åˆ°æ—§æ ¼å¼featuresï¼Œå°è¯•è½¬æ¢")
                    features = step1_output['features']
                    
                    # åˆå¹¶æ—§æ ¼å¼ç‰¹å¾ï¼ˆ40ç»´ï¼‰
                    feature_list = []
                    total_dim = 0
                    for name, tensor in features.items():
                        logger.info(f"   æ·»åŠ ç‰¹å¾ {name}: {tensor.shape[1]}ç»´")
                        total_dim += tensor.shape[1]
                        tensor = tensor.to(self.device)
                        feature_list.append(tensor)
                    input_features = torch.cat(feature_list, dim=1)  # [N, 40]
                    logger.info(f"   æ—§æ ¼å¼åˆå¹¶ç»“æœ: {input_features.shape}")
                else:
                    logger.error("âŒ [CRITICAL] Step1è¾“å‡ºæ ¼å¼ä¸å…¼å®¹")
                    raise ValueError("Step1å¿…é¡»æä¾›f_classicæˆ–features")
            
            # æ£€æŸ¥è¾¹ç´¢å¼•è¯¦æƒ…
            if edge_index is not None:
                logger.info(f"   è¾¹ç´¢å¼•: {edge_index.shape}, è¾¹æ•°{edge_index.shape[1]}")
            else:
                logger.warning("   âŒ Step1æœªæä¾›è¾¹ç´¢å¼•")
            
            # å‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆæŒ‰ç…§All_Final.pyçš„è¦æ±‚ï¼‰
            num_nodes = input_features.shape[0]
            logger.info(f"   èŠ‚ç‚¹æ€»æ•°: {num_nodes}")
            
            # === é‚»æ¥çŸ©é˜µæ„å»º ===
            logger.info("=== é‚»æ¥çŸ©é˜µæ„å»º ===")
            if edge_index is not None and edge_index.shape[1] > 0:
                logger.info(f"   ä½¿ç”¨Step1è¾¹ç´¢å¼•: {edge_index.shape[1]}æ¡è¾¹")
                
                # ä¼˜å…ˆä½¿ç”¨Step1çš„çœŸå®è¾¹ç´¢å¼•
                adjacency = torch.zeros(num_nodes, num_nodes, device=self.device)
                row, col = edge_index[0], edge_index[1]
                valid_mask = (row < num_nodes) & (col < num_nodes) & (row >= 0) & (col >= 0)
                
                if valid_mask.sum() > 0:
                    row, col = row[valid_mask], col[valid_mask]
                    adjacency[row, col] = 1.0
                    adjacency[col, row] = 1.0  # ç¡®ä¿å¯¹ç§°æ€§ï¼ˆæ— å‘å›¾ï¼‰
                    
                    # è®¡ç®—é‚»æ¥çŸ©é˜µç»Ÿè®¡ä¿¡æ¯
                    total_edges = adjacency.sum().item() // 2  # é™¤ä»¥2å› ä¸ºæ˜¯å¯¹ç§°çŸ©é˜µ
                    density = total_edges / (num_nodes * (num_nodes - 1) / 2)
                    
                    logger.info(f"    çœŸå®é‚»æ¥çŸ©é˜µ: {total_edges}è¾¹, å¯†åº¦{density:.4f}")
                else:
                    logger.warning("âš ï¸  [FALLBACK] è¾¹ç´¢å¼•æ— æ•ˆï¼Œä½¿ç”¨æ™ºèƒ½å¤‡ç”¨é‚»æ¥çŸ©é˜µ")
                    logger.warning("âš ï¸  [FALLBACK] è¿™ç¡®ä¿ç½‘ç»œè¿é€šæ€§ï¼Œç”Ÿäº§ç¯å¢ƒåº”æ£€æŸ¥è¾¹ç´¢å¼•ç”Ÿæˆé€»è¾‘")
                    adjacency = self._create_fallback_adjacency(num_nodes)
            else:
                logger.warning("âš ï¸  [FALLBACK] Step1æœªæä¾›è¾¹ç´¢å¼•ï¼Œåˆ›å»ºå¤‡ç”¨é‚»æ¥çŸ©é˜µ")
                logger.warning("âš ï¸  [FALLBACK] ä½¿ç”¨å°ä¸–ç•Œç½‘ç»œæ¨¡å‹ç¡®ä¿å›¾è¿é€šæ€§")
                adjacency = self._create_fallback_adjacency(num_nodes)
            
            # === TemporalMSCIAè°ƒç”¨ ===
            logger.info("=== TemporalMSCIAè°ƒç”¨ ===")
            
            # å‡†å¤‡batch_dataæ ¼å¼
            num_centers = min(32, num_nodes)
            center_indices = torch.randperm(num_nodes, device=self.device)[:num_centers]
            node_types = torch.randint(0, 5, (num_nodes,), device=self.device)
            
            batch_data = {
                'adjacency_matrix': adjacency,  # [N, N]
                'node_features': input_features,  # [N, 128] æˆ– [N, 40]
                'center_indices': center_indices,
                'node_types': node_types,
                'timestamp': 1
            }
            logger.info(f"   è¾“å…¥: {input_features.shape}ç‰¹å¾, {adjacency.shape}é‚»æ¥, {num_centers}ä¸­å¿ƒ")
            
            # è®°å½•æ¨ç†å¼€å§‹æ—¶é—´
            inference_start = time.time()
            self.step2_processor.train()
            output = self.step2_processor(batch_data)
            inference_time = time.time() - inference_start
            logger.info(f"   TemporalMSCIAæ¨ç†è€—æ—¶: {inference_time:.3f}ç§’")
            
            # === Step2è¾“å‡ºè§£æ ===
            logger.info("=== Step2 è¾“å‡ºè§£æ ===")
            
            # è§£æè¾“å‡º
            if isinstance(output, tuple):
                loss, embeddings = output
                final_loss = loss.item()
                logger.info(f"   æŸå¤±: {final_loss:.6f}, åµŒå…¥: {embeddings.shape}")
            elif isinstance(output, dict):
                final_loss = output.get('loss', output.get('total_loss', 0.0))
                if torch.is_tensor(final_loss):
                    final_loss = final_loss.item()
                embeddings = output.get('embeddings', output.get('node_embeddings'))
                logger.info(f"   æŸå¤±: {final_loss:.6f}, åµŒå…¥: {embeddings.shape}")
            else:
                final_loss = 0.0
                embeddings = output
                logger.info(f"   ç›´æ¥åµŒå…¥è¾“å‡º: {embeddings.shape}")
            
            # ç¡®ä¿åµŒå…¥æ ¼å¼æ­£ç¡®
            if embeddings is not None:
                if embeddings.dim() == 3:  # [1, N, hidden_dim]
                    embeddings = embeddings.squeeze(0)  # [N, hidden_dim]
                enhanced_features = embeddings
                logger.info(f"   æœ€ç»ˆåµŒå…¥: {enhanced_features.shape}, èŒƒå›´[{enhanced_features.min().item():.3f}, {enhanced_features.max().item():.3f}]")
            else:
                logger.warning("   âŒ æœªè·å¾—æœ‰æ•ˆåµŒå…¥ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾")
                enhanced_features = combined_features
            
            # ä¿å­˜ç»“æœ - å…¼å®¹Step3æ ¼å¼è¦æ±‚
            result = {
                # Step3éœ€è¦çš„æ ¸å¿ƒæ•°æ®
                'enhanced_features': enhanced_features,
                'embeddings': enhanced_features,
                'temporal_embeddings': enhanced_features,  # EvolveGCNå¯èƒ½æœŸæœ›è¿™ä¸ªé”®
                
                # å…ƒæ•°æ®
                'final_loss': final_loss,
                'embedding_dim': enhanced_features.shape[1],
                'num_nodes': enhanced_features.shape[0],
                'algorithm': 'Authentic_TemporalMSCIA_All_Final',
                'success': True,
                'processing_time': inference_time,
                
                # ä¼ é€’Step1çš„å¿…è¦æ•°æ®ç»™Step3
                'edge_index': step1_output.get('edge_index'),
                'adjacency_matrix': adjacency,
                'node_mapping': step1_output.get('node_mapping', {}),
                'metadata': {
                    'step2_processed': True,
                    'input_dim': input_features.shape[1],
                    'output_dim': enhanced_features.shape[1],
                    'edge_count': edge_index.shape[1] if edge_index is not None else 0,
                    'timestamp': int(time.time())
                }
            }
            
            # ä¿å­˜Step2ç»“æœæ–‡ä»¶
            step2_file = self.output_dir / "step2_multiscale.pkl"
            with open(step2_file, 'wb') as f:
                pickle.dump(result, f)
            
            logger.info("Step2å¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ å®Œæˆ")
            logger.info(f"   åµŒå…¥ç»´åº¦: {result.get('embedding_dim', 'Unknown')}")
            logger.info(f"   æŸå¤±å€¼: {result.get('final_loss', 'Unknown')}")
            logger.info(f" [STEP2â†’STEP3] æ•°æ®æ ¼å¼å·²å°±ç»ªï¼šæ—¶åºåµŒå…¥{enhanced_features.shape} + é‚»æ¥çŸ©é˜µ")
            
            return result
            
        except Exception as e:
            logger.error(f"Step2æ‰§è¡Œå¤±è´¥: {e}")
            raise RuntimeError(f"Step2æ‰§è¡Œå¤±è´¥ï¼Œä¸ä½¿ç”¨å¤‡ç”¨å®ç°: {e}")
    
    def run_step3_evolve_gcn(self, step1_output: Dict[str, Any], step2_output: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¿è¡Œç¬¬ä¸‰æ­¥ï¼šEvolveGCNåˆ†ç‰‡
        ç›´æ¥ä½¿ç”¨Step2çš„æ—¶åºåµŒå…¥å’ŒStep1çš„é‚»æ¥çŸ©é˜µï¼Œä¸è¿›è¡Œä»»ä½•æ ¼å¼è½¬æ¢
        
        Args:
            step1_output: Step1çš„è¾“å‡ºç»“æœï¼ŒåŒ…å«edge_index
            step2_output: Step2çš„è¾“å‡ºç»“æœï¼ŒåŒ…å«temporal_embeddings
            
        Returns:
            EvolveGCNåˆ†ç‰‡ç»“æœ
        """
        logger.info("æ‰§è¡ŒStep3ï¼šEvolveGCNåˆ†ç‰‡")
        
        try:
            # ç›´æ¥ä½¿ç”¨Step2çš„æ—¶åºåµŒå…¥ä½œä¸ºxè¾“å…¥
            temporal_embeddings = step2_output.get('enhanced_features')
            if temporal_embeddings is None:
                temporal_embeddings = step2_output.get('temporal_embeddings')
            if temporal_embeddings is None:
                raise ValueError("Step2æœªæä¾›æ—¶åºåµŒå…¥æ•°æ®")
            
            # ä½¿ç”¨Step1çš„é‚»æ¥çŸ©é˜µ
            edge_index = step1_output.get('edge_index')
            if edge_index is None:
                raise ValueError("Step1æœªæä¾›é‚»æ¥çŸ©é˜µ")
            
            # === è®°å½•è¾“å…¥æ•°æ®ä¿¡æ¯ ===
            logger.info("=== Step3 ç›´æ¥æ•°æ®ä¼ é€’ ===")
            logger.info(f"   æ—¶åºåµŒå…¥å½¢çŠ¶: {temporal_embeddings.shape}")
            logger.info(f"   æ—¶åºåµŒå…¥è®¾å¤‡: {temporal_embeddings.device}")
            logger.info(f"   è¾¹ç´¢å¼•å½¢çŠ¶: {edge_index.shape}")
            logger.info(f"   è¾¹ç´¢å¼•è®¾å¤‡: {edge_index.device}")
            
            # ç¡®ä¿æ•°æ®ç±»å‹å’Œè®¾å¤‡ä¸€è‡´æ€§
            import torch
            if not isinstance(temporal_embeddings, torch.Tensor):
                temporal_embeddings = torch.tensor(temporal_embeddings, dtype=torch.float32, device=self.device)
            else:
                temporal_embeddings = temporal_embeddings.to(self.device)
                
            if not isinstance(edge_index, torch.Tensor):
                edge_index = torch.tensor(edge_index, dtype=torch.long, device=self.device)
            else:
                edge_index = edge_index.to(self.device)
            
            logger.info(f"   è¾“å…¥è®¾å¤‡ç»Ÿä¸€åˆ°: {self.device}")
            
            # ç›´æ¥è°ƒç”¨EvolveGCNWrapper.forwardæ–¹æ³•
            logger.info(" [STEP3] è°ƒç”¨EvolveGCNWrapper.forward()")
            evolve_start = time.time()
            
            embeddings, delta_signal = self.step3_processor.forward(
                x=temporal_embeddings,  # Step2çš„æ—¶åºåµŒå…¥
                edge_index=edge_index,  # Step1çš„é‚»æ¥çŸ©é˜µ
                performance_feedback=None
            )
            
            evolve_time = time.time() - evolve_start
            logger.info(f" [STEP3] EvolveGCNå‰å‘ä¼ æ’­å®Œæˆï¼Œè€—æ—¶: {evolve_time:.3f}ç§’")
            
            # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
            device = next(self.step3_processor.parameters()).device
            logger.info(f"   EvolveGCNæ¨¡å‹è®¾å¤‡: {device}")
            
            # ä½¿ç”¨embeddingsä½œä¸ºenhanced_featuresï¼ˆè¿™æ˜¯EvolveGCNçš„è¾“å‡ºï¼‰
            enhanced_features = embeddings.to(device)
            
            if edge_index is not None:
                edge_index = edge_index.to(device)
                logger.info(f"   è¾“å…¥å¼ é‡å·²ç§»è‡³è®¾å¤‡: {device}")
            else:
                logger.warning("âš ï¸ [WARNING] è¾¹ç´¢å¼•ä¸ºç©ºï¼ŒEvolveGCNå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
                # ä¸ºEvolveGCNåˆ›å»ºä¸€ä¸ªæœ€å°çš„è¾¹ç´¢å¼•
                num_nodes = enhanced_features.shape[0]
                edge_index = torch.stack([
                    torch.arange(num_nodes-1), 
                    torch.arange(1, num_nodes)
                ], dim=0).to(device)
                logger.warning(f"   åˆ›å»ºæœ€å°è¾¹ç´¢å¼•: {edge_index.shape}")
            
            # è®°å½•EvolveGCNæ¨ç†æ—¶é—´ï¼ˆåˆ é™¤é‡å¤çš„forwardè°ƒç”¨ï¼‰
            # embeddingså’Œdelta_signalå·²ç»ä»ä¸Šé¢çš„forwardè°ƒç”¨ä¸­è·å¾—
            
            # === è¯¦ç»†è®°å½•EvolveGCNè¾“å‡º ===
            logger.info("=== EvolveGCN è¾“å‡ºè¯¦æƒ… ===")
            logger.info(f"   åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
            logger.info(f"   å¢é‡ä¿¡å·å½¢çŠ¶: {delta_signal.shape}")
            
            # è®°å½•è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            logger.info(f"   è¾“å‡ºåµŒå…¥å½¢çŠ¶: {embeddings.shape}")
            logger.info(f"   è¾“å‡ºåµŒå…¥è®¾å¤‡: {embeddings.device}")
            logger.info(f"   deltaä¿¡å·å½¢çŠ¶: {delta_signal.shape}")
            
            emb_stats = {
                'min': embeddings.min().item(),
                'max': embeddings.max().item(), 
                'mean': embeddings.mean().item(),
                'std': embeddings.std().item()
            }
            logger.info(f"   åµŒå…¥ç»Ÿè®¡: {emb_stats}")
            
            # === çœŸæ­£çš„EvolveGCNåˆ†ç‰‡ç®—æ³• ===
            logger.info("=== çœŸæ­£çš„EvolveGCNåˆ†ç‰‡ç®—æ³• ===")
            
            try:
                # å¯¼å…¥çœŸæ­£çš„DynamicShardingModule
                sys.path.append(str(Path(__file__).parent / "evolve_GCN" / "models"))
                from sharding_modules import DynamicShardingModule
                
                logger.info(" æˆåŠŸå¯¼å…¥çœŸæ­£çš„DynamicShardingModule")
                
                # åˆå§‹åŒ–çœŸæ­£çš„åŠ¨æ€åˆ†ç‰‡æ¨¡å—
                embedding_dim = embeddings.shape[1]
                
                dynamic_sharding = DynamicShardingModule(
                    embedding_dim=embedding_dim,
                    base_shards=4,
                    max_shards=8
                ).to(self.device)
                
                logger.info(f"   DynamicShardingModuleåˆå§‹åŒ–: è¾“å…¥ç»´åº¦={embedding_dim}")
                
                # æ‰§è¡ŒçœŸæ­£çš„EvolveGCNåˆ†ç‰‡
                history_states = []  # é¦–æ¬¡è¿è¡Œä½¿ç”¨ç©ºå†å²
                feedback_signal = None  # é¦–æ¬¡è¿è¡Œæ— åé¦ˆ
                
                logger.info(" [STEP3] æ‰§è¡ŒçœŸæ­£çš„DynamicShardingModuleåˆ†ç‰‡...")
                shard_start = time.time()
                
                shard_assignments, enhanced_embeddings, attention_weights, predicted_num_shards = dynamic_sharding(
                    embeddings, 
                    history_states=history_states,
                    feedback_signal=feedback_signal
                )
                
                shard_time = time.time() - shard_start
                logger.info(f" [STEP3] çœŸæ­£çš„EvolveGCNåˆ†ç‰‡å®Œæˆï¼Œè€—æ—¶: {shard_time:.3f}ç§’")
                
                # ç”Ÿæˆç¡¬åˆ†é…
                hard_assignment = torch.argmax(shard_assignments, dim=1)
                unique_shards, shard_counts = torch.unique(hard_assignment, return_counts=True)
                
                # åˆ†æåˆ†ç‰‡åˆ†é…è´¨é‡
                shard_assignments_np = hard_assignment.cpu().numpy()
                shard_count_dict = dict(zip(unique_shards.cpu().numpy(), shard_counts.cpu().numpy()))
                logger.info(f"   çœŸå®åˆ†ç‰‡åˆ†å¸ƒ: {shard_count_dict}")
                logger.info(f"   é¢„æµ‹åˆ†ç‰‡æ•°: {predicted_num_shards}")
                logger.info(f"   å®é™…ä½¿ç”¨åˆ†ç‰‡: {len(unique_shards)}")
                
                # è®¡ç®—è´Ÿè½½å‡è¡¡åº¦
                if len(shard_counts) > 1:
                    load_balance = 1.0 - (shard_counts.max() - shard_counts.min()).float() / shard_counts.float().mean()
                else:
                    load_balance = 1.0
                logger.info(f"   è´Ÿè½½å‡è¡¡åº¦: {load_balance:.3f}")
                
                # æ„å»ºåˆ†ç‰‡ç»“æœ
                result = {
                    'embeddings': enhanced_embeddings.detach().cpu().numpy(),
                    'delta_signal': delta_signal.detach().cpu().numpy(), 
                    'shard_assignments': shard_assignments_np.tolist(),
                    'num_shards': int(predicted_num_shards),
                    'assignment_quality': float(load_balance),
                    'algorithm': 'Real-EvolveGCN-Dynamic-Sharding',
                    'authentic_implementation': True,
                    'sharding_time': shard_time,
                    'attention_weights': attention_weights.detach().cpu().numpy() if attention_weights is not None else None,
                    'predicted_shards': int(predicted_num_shards),
                    'actual_shards': len(unique_shards)
                }
                
                logger.info(" [STEP3] çœŸæ­£çš„EvolveGCNåˆ†ç‰‡ç®—æ³•æ‰§è¡ŒæˆåŠŸ")
                
            except Exception as e:
                logger.error(f"âŒ [CRITICAL] çœŸæ­£çš„EvolveGCNåˆ†ç‰‡å¤±è´¥: {e}")
                logger.error("âŒ [CRITICAL] ç³»ç»Ÿæ‹’ç»é™çº§åˆ°ç®€åŒ–å®ç°")
                raise RuntimeError(f"EvolveGCNåˆ†ç‰‡å¤±è´¥ï¼Œæ‹’ç»ä½¿ç”¨ç®€åŒ–å®ç°: {e}")
            
            # === æ—§çš„ç®€å•åˆ†ç‰‡ç®—æ³•å·²åˆ é™¤ ===
            
            # ä¿å­˜ç»“æœ
            step3_file = self.output_dir / "step3_sharding.pkl"
            with open(step3_file, 'wb') as f:
                pickle.dump(result, f)
            
            logger.info(" Step3 EvolveGCNåˆ†ç‰‡å®Œæˆ")
            logger.info(f"   åˆ†ç‰‡æ•°é‡: {result.get('num_shards')}")
            logger.info(f"   åˆ†é…è´¨é‡: {result.get('assignment_quality'):.3f}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Step3æ‰§è¡Œå¤±è´¥: {e}")
            raise RuntimeError(f"Step3æ‰§è¡Œå¤±è´¥: {e}")
    
    def run_step4_feedback(self, step1_output: Dict[str, Any], step3_output: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¿è¡Œç¬¬å››æ­¥ï¼šç»Ÿä¸€åé¦ˆå¼•æ“
        
        Args:
            step1_output: Step1çš„è¾“å‡ºç»“æœ
            step3_output: Step3çš„è¾“å‡ºç»“æœ
            
        Returns:
            åé¦ˆä¼˜åŒ–ç»“æœ
        """
        logger.info("æ‰§è¡ŒStep4ï¼šç»Ÿä¸€åé¦ˆå¼•æ“")
        
        try:
            #  [STEP4_OPTIMIZATION] ç›´æ¥ä½¿ç”¨Step1æä¾›çš„6ç±»ç‰¹å¾å­—å…¸
            # Step1ç°åœ¨æä¾›ä¸¤ç§æ ¼å¼ï¼šf_classic(ç»™Step2) + features_dict(ç»™Step4)
            
            features = None
            
            # ä¼˜å…ˆä½¿ç”¨Step1çš„6ç±»ç‰¹å¾å­—å…¸
            if 'features_dict' in step1_output and step1_output['features_dict'] is not None:
                features = step1_output['features_dict']
                logger.info(" [STEP4] ä½¿ç”¨Step1æä¾›çš„6ç±»ç‰¹å¾å­—å…¸")
                logger.info(f"   ç‰¹å¾ç±»åˆ«: {list(features.keys())}")
                for name, tensor in features.items():
                    if isinstance(tensor, torch.Tensor):
                        logger.info(f"   {name}: {tensor.shape}, è®¾å¤‡: {tensor.device}")
                        logger.info(f"   {name} ç»Ÿè®¡: [{tensor.min().item():.3f}, {tensor.max().item():.3f}]")
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä»f_classicåˆ†è§£ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            elif 'f_classic' in step1_output and step1_output['f_classic'] is not None:
                logger.warning("âš ï¸ [STEP4] Step1æœªæä¾›6ç±»ç‰¹å¾å­—å…¸ï¼Œä»f_classicåˆ†è§£")
                f_classic = step1_output['f_classic']  # [N, 80] æˆ–å…¶ä»–ç»´åº¦
                num_nodes = f_classic.shape[0]
                feature_dim = f_classic.shape[1]
                
                logger.info(f"   Step1 f_classicå½¢çŠ¶: {f_classic.shape}")
                
                # æ ¹æ®å®é™…ç»´åº¦æ™ºèƒ½åˆ†å‰²ä¸º6ç±»ç‰¹å¾
                if feature_dim >= 80:  # æ‰©å±•åçš„f_classic
                    # åŸºäºç»Ÿä¸€åé¦ˆå¼•æ“æœŸæœ›çš„ç»´åº¦åˆ†é…ï¼ˆæ€»è®¡48ç»´ï¼‰
                    features = {
                        'hardware': f_classic[:, :11].to(self.device),           # å‰11ç»´ï¼šç¡¬ä»¶ç‰¹å¾
                        'onchain_behavior': f_classic[:, 11:26].to(self.device), # 12-26ç»´ï¼šé“¾ä¸Šè¡Œä¸º
                        'network_topology': f_classic[:, 26:31].to(self.device), # 27-31ç»´ï¼šç½‘ç»œæ‹“æ‰‘  
                        'dynamic_attributes': f_classic[:, 31:38].to(self.device), # 32-38ç»´ï¼šåŠ¨æ€å±æ€§
                        'heterogeneous_type': f_classic[:, 38:40].to(self.device), # 39-40ç»´ï¼šå¼‚æ„ç±»å‹
                        'categorical': f_classic[:, 40:48].to(self.device)       # 41-48ç»´ï¼šåˆ†ç±»ç‰¹å¾
                    }
                elif feature_dim >= 40:  # åŸå§‹40ç»´ç‰¹å¾
                    # æŒ‰ç…§ç»Ÿä¸€åé¦ˆå¼•æ“æœŸæœ›çš„40ç»´åˆ†å¸ƒï¼ˆæ— categoricalï¼‰
                    features = {
                        'hardware': f_classic[:, :11].to(self.device),           # å‰11ç»´ï¼šç¡¬ä»¶ç‰¹å¾
                        'onchain_behavior': f_classic[:, 11:26].to(self.device), # 12-26ç»´ï¼šé“¾ä¸Šè¡Œä¸º
                        'network_topology': f_classic[:, 26:31].to(self.device), # 27-31ç»´ï¼šç½‘ç»œæ‹“æ‰‘
                        'dynamic_attributes': f_classic[:, 31:38].to(self.device), # 32-38ç»´ï¼šåŠ¨æ€å±æ€§
                        'heterogeneous_type': f_classic[:, 38:40].to(self.device), # 39-40ç»´ï¼šå¼‚æ„ç±»å‹
                        # categoricalç‰¹å¾ä½¿ç”¨é›¶å¡«å……ï¼Œå› ä¸º40ç»´è¾“å…¥æ²¡æœ‰è¿™ä¸ªç±»åˆ«
                        'categorical': torch.zeros(num_nodes, 8, device=self.device)
                    }
                else:
                    # ç»´åº¦ä¸è¶³ï¼Œä½¿ç”¨å‡åŒ€åˆ†é…
                    logger.warning(f"   f_classicç»´åº¦ä¸è¶³({feature_dim})ï¼Œä½¿ç”¨å‡åŒ€åˆ†é…")
                    dim_per_category = feature_dim // 6
                    features = {
                        'hardware': f_classic[:, :dim_per_category].to(self.device),
                        'onchain_behavior': f_classic[:, dim_per_category:2*dim_per_category].to(self.device),
                        'network_topology': f_classic[:, 2*dim_per_category:3*dim_per_category].to(self.device),
                        'dynamic_attributes': f_classic[:, 3*dim_per_category:4*dim_per_category].to(self.device),
                        'heterogeneous_type': f_classic[:, 4*dim_per_category:5*dim_per_category].to(self.device),
                        'categorical': f_classic[:, 5*dim_per_category:].to(self.device)
                    }
                
                logger.info("    æˆåŠŸå°†Step1è¾“å‡ºè½¬æ¢ä¸º6ç±»ç‰¹å¾æ ¼å¼")
                
            else:
                # æœ€åå¤‡ç”¨æ–¹æ¡ˆï¼šä»Step3çš„embeddingsç”Ÿæˆç‰¹å¾
                logger.warning("   âš ï¸ Step1æœªæä¾›f_classicï¼Œä½¿ç”¨Step3 embeddingså¤‡ç”¨æ–¹æ¡ˆ")
                if 'embeddings' in step3_output:
                    embeddings = step3_output['embeddings']  # [N, 128]
                    if isinstance(embeddings, np.ndarray):
                        embeddings = torch.tensor(embeddings, device=self.device)
                    num_nodes = embeddings.shape[0]
                    embed_dim = embeddings.shape[1]
                    
                    # ä»embeddingsä¸­åˆ†å‰²å‡º6ç±»ç‰¹å¾
                    dim_per_category = embed_dim // 6
                    features = {
                        'hardware': embeddings[:, :dim_per_category].to(self.device),
                        'onchain_behavior': embeddings[:, dim_per_category:2*dim_per_category].to(self.device),
                        'network_topology': embeddings[:, 2*dim_per_category:3*dim_per_category].to(self.device),
                        'dynamic_attributes': embeddings[:, 3*dim_per_category:4*dim_per_category].to(self.device),
                        'heterogeneous_type': embeddings[:, 4*dim_per_category:5*dim_per_category].to(self.device),
                        'categorical': embeddings[:, 5*dim_per_category:].to(self.device)
                    }
                    logger.info("    ä»Step3 embeddingsç”Ÿæˆ6ç±»ç‰¹å¾")
                else:
                    raise KeyError("Step1è¾“å‡ºä¸­æ—¢æ— features_dictä¹Ÿæ— f_classicï¼Œä¸”Step3æ— embeddingsï¼Œæ— æ³•ä¸ºStep4æä¾›ç‰¹å¾")
            
            # === è¯¦ç»†è®°å½•Step4è¾“å…¥å‚æ•° ===
            logger.info("=== Step4 è¾“å…¥å‚æ•°è¯¦æƒ… ===")
            logger.info(f"   ç‰¹å¾æ¥æº: {'Step1_features_dict' if 'features_dict' in step1_output else 'f_classic_decomposition' if 'f_classic' in step1_output else 'Step3_embeddings'}")
            logger.info(f"   ç‰¹å¾ç±»åˆ«: {list(features.keys())}")
            logger.info(f"   Step3è¾“å‡ºé”®: {list(step3_output.keys())}")
            
            # ä½¿ç”¨çœŸå®ç»Ÿä¸€åé¦ˆå¼•æ“çš„process_sharding_feedbackæ–¹æ³•
            if hasattr(self.step4_processor, 'process_sharding_feedback'):
                logger.info("    ä½¿ç”¨çœŸå®UnifiedFeedbackEngine.process_sharding_feedbackæ–¹æ³•")
                
                # ä»Step3è¾“å‡ºä¸­æå–åˆ†ç‰‡åˆ†é…ç»“æœ
                shard_assignments = step3_output.get('shard_assignments', None)
                if shard_assignments is None and 'sharding_assignments' in step3_output:
                    shard_assignments = step3_output['sharding_assignments']
                
                if shard_assignments is None:
                    raise ValueError("Step3è¾“å‡ºä¸­æœªæ‰¾åˆ°åˆ†ç‰‡åˆ†é…ç»“æœ")
                
                # === è®°å½•åˆ†ç‰‡åˆ†é…è¯¦æƒ… ===
                logger.info("=== åˆ†ç‰‡åˆ†é…åˆ†æ ===")
                if isinstance(shard_assignments, list):
                    shard_array = np.array(shard_assignments)
                    logger.info(f"   åˆ†ç‰‡åˆ†é…é•¿åº¦: {len(shard_assignments)}")
                    logger.info(f"   åˆ†ç‰‡IDèŒƒå›´: [{shard_array.min()}, {shard_array.max()}]")
                    unique_shards, counts = np.unique(shard_array, return_counts=True)
                    logger.info(f"   åˆ†ç‰‡åˆ†å¸ƒ: {dict(zip(unique_shards.tolist(), counts.tolist()))}")
                
                # ç¡®ä¿åˆ†ç‰‡åˆ†é…æ˜¯tensoræ ¼å¼
                if not isinstance(shard_assignments, torch.Tensor):
                    shard_assignments = torch.tensor(shard_assignments, device=self.device)
                    logger.info("   è½¬æ¢åˆ†ç‰‡åˆ†é…ä¸ºå¼ é‡")
                
                logger.info(f"   åˆ†ç‰‡åˆ†é…å¼ é‡å½¢çŠ¶: {shard_assignments.shape}")
                logger.info(f"   åˆ†ç‰‡åˆ†é…è®¾å¤‡: {shard_assignments.device}")
                
                # === è®°å½•åé¦ˆå¼•æ“å¤„ç†æ—¶é—´ ===
                feedback_start = time.time()
                
                # è°ƒç”¨çœŸå®ç»Ÿä¸€åé¦ˆå¼•æ“
                result = self.step4_processor.process_sharding_feedback(
                    features=features,
                    shard_assignments=shard_assignments,
                    edge_index=None,  # å¯é€‰çš„è¾¹ç´¢å¼•
                    performance_hints=step3_output.get('performance_metrics', None)
                )
                
                feedback_time = time.time() - feedback_start
                logger.info(f"   åé¦ˆå¤„ç†è€—æ—¶: {feedback_time:.3f}ç§’")
                
                # === è¯¦ç»†è®°å½•åé¦ˆå¼•æ“è¾“å‡º ===
                logger.info("=== åé¦ˆå¼•æ“è¾“å‡ºè¯¦æƒ… ===")
                logger.info(f"   ç»“æœç±»å‹: {type(result)}")
                if isinstance(result, dict):
                    logger.info(f"   è¾“å‡ºé”®: {list(result.keys())}")
                    for key, value in result.items():
                        if isinstance(value, (int, float)):
                            logger.info(f"   {key}: {value}")
                        elif isinstance(value, torch.Tensor):
                            logger.info(f"   {key}: å½¢çŠ¶ {value.shape}, è®¾å¤‡ {value.device}")
                        elif isinstance(value, list):
                            logger.info(f"   {key}: åˆ—è¡¨é•¿åº¦ {len(value)}")
                        elif isinstance(value, dict):
                            logger.info(f"   {key}: å­—å…¸ï¼Œé”® {list(value.keys())}")
                        else:
                            logger.info(f"   {key}: {str(value)[:100]}")
                
            else:
                raise RuntimeError("Step4å¤„ç†å™¨ç¼ºå°‘process_sharding_feedbackæ–¹æ³•")
            
            # ä¿å­˜ç»“æœ
            step4_file = self.output_dir / "step4_feedback.pkl"
            with open(step4_file, 'wb') as f:
                pickle.dump(result, f)
            
            logger.info("Step4ç»Ÿä¸€åé¦ˆå¼•æ“å®Œæˆ")
            logger.info(f"   ç»¼åˆè¯„åˆ†: {result.get('optimized_feedback', {}).get('overall_score', 'Unknown')}")
            logger.info(f"   æ™ºèƒ½å»ºè®®: {len(result.get('smart_suggestions', []))} é¡¹")
            logger.info(f"   å¼‚å¸¸æ£€æµ‹: {len(result.get('anomaly_report', {}).get('detected_anomalies', []))} ä¸ªå¼‚å¸¸")
            
            return result
            
        except Exception as e:
            logger.error(f"Step4æ‰§è¡Œå¤±è´¥: {e}")
            raise RuntimeError(f"Step4æ‰§è¡Œå¤±è´¥ï¼Œä¸ä½¿ç”¨å¤‡ç”¨å®ç°: {e}")
    
    def run_complete_pipeline(self, node_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„å››æ­¥åˆ†ç‰‡æµæ°´çº¿
        
        Args:
            node_data: èŠ‚ç‚¹æ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å®Œæ•´æµæ°´çº¿çš„ç»“æœ
        """
        logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´å››æ­¥åˆ†ç‰‡æµæ°´çº¿")
        start_time = time.time()
        
        try:
            # ç¡®ä¿æ‰€æœ‰ç»„ä»¶å·²åˆå§‹åŒ–
            if not all([self.step1_processor, self.step2_processor, self.step3_processor, self.step4_processor]):
                logger.info("ç»„ä»¶æœªå®Œå…¨åˆå§‹åŒ–ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
                self.initialize_all_components()
            
            # Step 1: ç‰¹å¾æå–
            step1_result = self.run_step1_feature_extraction(node_data)
            
            # Step 2: å¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ 
            step2_result = self.run_step2_multiscale_learning(step1_result)
            
            # Step 3: EvolveGCNåˆ†ç‰‡
            step3_result = self.run_step3_evolve_gcn(step1_result, step2_result)
            
            # Step 4: ç»Ÿä¸€åé¦ˆå¼•æ“
            step4_result = self.run_step4_feedback(step1_result, step3_result)
            
            # æ•´åˆæœ€ç»ˆç»“æœ
            final_result = {
                'success': True,
                'execution_time': time.time() - start_time,
                'step1_features': step1_result,
                'step2_multiscale': step2_result,
                'step3_sharding': step3_result,
                'step4_feedback': step4_result,
                
                # BlockEmulatoræ¥å£å…¼å®¹çš„è¾“å‡ºæ ¼å¼
                'shard_assignments': step3_result.get('shard_assignments'),
                'num_shards': step3_result.get('num_shards'),
                'performance_score': step4_result.get('quality_score', 0.5),
                'algorithm': 'Complete_Integrated_Four_Step_EvolveGCN',
                'feature_count': sum(self.real_feature_dims.values()),
                'metadata': {
                    'real_44_fields': True,
                    'authentic_multiscale': True,
                    'authentic_evolvegcn': True,
                    'unified_feedback': True,
                    # é‡è¦ï¼šä¼ é€’åŸå§‹èŠ‚ç‚¹æ˜ å°„ä¿¡æ¯ - ä¿®å¤è·¯å¾„
                    'node_info': step1_result.get('node_info', {}),  # ç›´æ¥ä»step1_resultè·å–
                    'original_node_mapping': step1_result.get('metadata', {}).get('original_node_mapping', {}),
                    'cross_shard_edges': step3_result.get('cross_shard_edges', 0)
                }
            }
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            final_file = self.output_dir / "complete_pipeline_result.pkl"
            with open(final_file, 'wb') as f:
                pickle.dump(final_result, f)
            
            # ä¿å­˜JSONæ ¼å¼ï¼ˆå¯è¯»ï¼‰
            json_result = self._convert_to_json_serializable(final_result)
            json_file = self.output_dir / "complete_pipeline_result.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_result, f, indent=2, ensure_ascii=False)
            
            logger.info("å®Œæ•´å››æ­¥åˆ†ç‰‡æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ")
            logger.info(f"   æ€»æ‰§è¡Œæ—¶é—´: {final_result['execution_time']:.2f}ç§’")
            logger.info(f"   åˆ†ç‰‡æ•°é‡: {final_result.get('num_shards', 'Unknown')}")
            logger.info(f"   æ€§èƒ½è¯„åˆ†: {final_result.get('performance_score', 'Unknown')}")
            logger.info(f"   ç»“æœæ–‡ä»¶: {final_file}")
            
            return final_result
            
        except Exception as e:
            logger.error(f"å®Œæ•´æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'success': False,
                'error': str(e),
                'execution_time': time.time() - start_time,
                'algorithm': 'Complete_Integrated_Four_Step_EvolveGCN_Failed'
            }
    
    def integrate_with_blockemulator(self, pipeline_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°†åˆ†ç‰‡ç»“æœé›†æˆåˆ°BlockEmulator
        
        Args:
            pipeline_result: å®Œæ•´æµæ°´çº¿çš„ç»“æœ
            
        Returns:
            BlockEmulatoré›†æˆç»“æœ
        """
        logger.info("å°†åˆ†ç‰‡ç»“æœé›†æˆåˆ°BlockEmulator")
        
        try:
            # å‡†å¤‡BlockEmulatoræ¥å£æ•°æ®
            integration_data = {
                'sharding_config': {
                    'shard_assignments': pipeline_result.get('shard_assignments'),
                    'num_shards': pipeline_result.get('num_shards'),
                    'performance_score': pipeline_result.get('performance_score'),
                    'algorithm_used': pipeline_result.get('algorithm')
                },
                'performance_metrics': pipeline_result.get('step4_feedback', {}).get('performance_metrics', {}),
                'smart_suggestions': pipeline_result.get('step4_feedback', {}).get('smart_suggestions', []),
                'metadata': pipeline_result.get('metadata', {}),
                'timestamp': time.time()
            }
            
            # ä¿å­˜é›†æˆé…ç½®
            integration_file = self.output_dir / "blockemulator_integration.json"
            with open(integration_file, 'w', encoding='utf-8') as f:
                json.dump(integration_data, f, indent=2, ensure_ascii=False)
            
            logger.info("BlockEmulatoré›†æˆé…ç½®å·²ç”Ÿæˆ")
            logger.info(f"   é›†æˆæ–‡ä»¶: {integration_file}")
            
            return integration_data
            
        except Exception as e:
            logger.error(f"BlockEmulatoré›†æˆå¤±è´¥: {e}")
            return {'error': str(e)}
    
    # === è¾…åŠ©æ–¹æ³• ===
    
    def _create_fallback_adjacency(self, num_nodes):
        """åˆ›å»ºå¤‡ç”¨é‚»æ¥çŸ©é˜µï¼ˆæ›´æ™ºèƒ½çš„è¿æ¥ç­–ç•¥ï¼‰"""
        logger.info(" [FALLBACK] åˆ›å»ºæ™ºèƒ½å¤‡ç”¨é‚»æ¥çŸ©é˜µ...")
        logger.info(" [FALLBACK] ä½¿ç”¨ç­–ç•¥ï¼šç¯å½¢è¿æ¥ + å°ä¸–ç•Œç½‘ç»œ + å±€éƒ¨è¿æ¥")
        
        adjacency = torch.zeros(num_nodes, num_nodes, device=self.device)
        
        # ç­–ç•¥1: ç¯å½¢è¿æ¥ç¡®ä¿è¿é€šæ€§
        logger.debug(" [FALLBACK] ç­–ç•¥1ï¼šåˆ›å»ºç¯å½¢è¿æ¥ç¡®ä¿åŸºæœ¬è¿é€šæ€§")
        for i in range(num_nodes):
            next_node = (i + 1) % num_nodes
            adjacency[i, next_node] = 1.0
            adjacency[next_node, i] = 1.0
        
        # ç­–ç•¥2: å°ä¸–ç•Œç½‘ç»œ - æ·»åŠ å°‘é‡é•¿è·ç¦»è¿æ¥
        num_long_edges = max(1, num_nodes // 10)
        logger.debug(f" [FALLBACK] ç­–ç•¥2ï¼šæ·»åŠ {num_long_edges}æ¡é•¿è·ç¦»è¿æ¥ï¼ˆå°ä¸–ç•Œç‰¹æ€§ï¼‰")
        for _ in range(num_long_edges):
            i = torch.randint(0, num_nodes, (1,)).item()
            j = torch.randint(0, num_nodes, (1,)).item()
            if i != j:
                adjacency[i, j] = 1.0
                adjacency[j, i] = 1.0
        
        # ç­–ç•¥3: åŸºäºè·ç¦»çš„å±€éƒ¨è¿æ¥
        logger.debug(" [FALLBACK] ç­–ç•¥3ï¼šåˆ›å»ºå±€éƒ¨é‚»åŸŸè¿æ¥")
        for i in range(num_nodes):
            # æ¯ä¸ªèŠ‚ç‚¹è¿æ¥åˆ°2-3ä¸ªé‚»è¿‘èŠ‚ç‚¹
            for offset in [2, 3]:
                if i + offset < num_nodes:
                    adjacency[i, i + offset] = 1.0
                    adjacency[i + offset, i] = 1.0
        
        # ç¡®ä¿æ— è‡ªç¯
        adjacency.fill_diagonal_(0)
        
        total_edges = adjacency.sum().item() // 2
        density = total_edges / (num_nodes * (num_nodes - 1) / 2)
        logger.info(f" [FALLBACK] å¤‡ç”¨é‚»æ¥çŸ©é˜µåˆ›å»ºå®Œæˆï¼š{num_nodes}èŠ‚ç‚¹, {total_edges}è¾¹, å¯†åº¦{density:.4f}")
        logger.info(" [FALLBACK] å¤‡ç”¨ç½‘ç»œç¡®ä¿äº†è¿é€šæ€§å’Œå°ä¸–ç•Œç‰¹æ€§ï¼Œæ»¡è¶³GCNå¤„ç†è¦æ±‚")
        
        return adjacency

    def _save_features_for_step2(self, features: Dict[str, torch.Tensor], feature_file: Path, adjacency_file: Path):
        """ä¿å­˜ç‰¹å¾æ–‡ä»¶ä¾›Step2ä½¿ç”¨"""
        try:
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾ä¸ºä¸€ä¸ªå¼ é‡
            feature_list = []
            for name, tensor in features.items():
                feature_list.append(tensor)
            
            combined_features = torch.cat(feature_list, dim=1)  # [N, 99]
            
            # ä¿å­˜CSVæ–‡ä»¶
            import pandas as pd
            df = pd.DataFrame(combined_features.cpu().numpy())
            df.to_csv(feature_file, index=False)
            
            # ç”Ÿæˆé‚»æ¥çŸ©é˜µ
            num_nodes = combined_features.shape[0]
            edge_index = torch.randint(0, num_nodes, (2, num_nodes * 4))
            
            # ä¿å­˜é‚»æ¥çŸ©é˜µ
            torch.save(edge_index, adjacency_file)
            
            logger.info(f"ç‰¹å¾æ–‡ä»¶å·²ä¿å­˜: {feature_file}")
            logger.info(f"é‚»æ¥æ–‡ä»¶å·²ä¿å­˜: {adjacency_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç‰¹å¾æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def _validate_step1_output(self, result: Dict[str, Any]):
        """éªŒè¯Step1è¾“å‡ºæ ¼å¼ - ä¼˜åŒ–ç‰ˆï¼šåªæ£€æŸ¥æ ¸å¿ƒç‰¹å¾"""
        logger.info(" [VALIDATION] éªŒè¯Step1è¾“å‡ºæ ¼å¼")
        
        # æ£€æŸ¥MainPipelineæ ‡å‡†æ ¼å¼ - åªéœ€è¦f_classic
        if 'f_classic' in result:
            logger.info(" [VALIDATION] æ£€æµ‹åˆ°MainPipelineæ ‡å‡†æ ¼å¼")
            
            f_classic = result['f_classic']
            
            # éªŒè¯f_classicç»´åº¦ï¼ˆè¿™æ˜¯Step2å’ŒStep3çš„å”¯ä¸€è¾“å…¥ï¼‰
            if f_classic is not None:
                expected_classic_dim = self.f_classic_dim  # ä½¿ç”¨å®é™…é…ç½®çš„ç»´åº¦ï¼ˆ80ç»´ï¼‰
                if f_classic.shape[1] != expected_classic_dim:
                    logger.warning(f"âš ï¸ [VALIDATION] f_classicç»´åº¦å¼‚å¸¸ï¼šæœŸæœ›{expected_classic_dim}ï¼Œå®é™…{f_classic.shape[1]}")
                    # ä¸ä½œä¸ºé”™è¯¯ï¼Œå…è®¸ç»§ç»­
                logger.info(f" [VALIDATION] f_classicç»´åº¦æ­£ç¡®ï¼š{f_classic.shape}")
            else:
                logger.error("âŒ [VALIDATION] f_classicä¸ºNone")
                return False
            
            # å¯é€‰ï¼šéªŒè¯f_graphç»´åº¦ï¼ˆä¸å½±å“æµç¨‹ï¼‰
            f_graph = result.get('f_graph')
            if f_graph is not None:
                if isinstance(f_graph, torch.Tensor) and f_graph.shape[1] != 96:
                    logger.warning(f"âš ï¸ [VALIDATION] f_graphç»´åº¦å¼‚å¸¸ï¼šæœŸæœ›96ï¼Œå®é™…{f_graph.shape[1]}")
                else:
                    logger.info(f" [VALIDATION] f_graphç»´åº¦æ­£ç¡®ï¼š{f_graph.shape}")
            else:
                logger.info(" [VALIDATION] f_graphä¸ºNoneï¼ˆä¼˜åŒ–è·³è¿‡å›¾ç‰¹å¾ç”Ÿæˆï¼‰")
            
            # å¯é€‰ï¼šéªŒè¯f_fusedï¼ˆä¸å½±å“æµç¨‹ï¼‰
            f_fused = result.get('f_fused')
            if f_fused is not None:
                logger.info(f" [VALIDATION] f_fusedå­˜åœ¨ï¼š{f_fused.shape}")
            else:
                logger.info(" [VALIDATION] æœªç”Ÿæˆf_fusedï¼ˆä¼˜åŒ–æ¨¡å¼ï¼‰")
            
            return True
            
        # å¤‡ç”¨ï¼šæ£€æŸ¥æ—§æ ¼å¼ï¼ˆåˆ†ç±»ç‰¹å¾æ ¼å¼ï¼‰
        elif 'features' in result:
            logger.info(" [VALIDATION] æ£€æµ‹åˆ°æ—§æ ¼å¼ç‰¹å¾ï¼Œè¿›è¡Œå…¼å®¹æ€§éªŒè¯")
            
            features = result['features']
            for feature_name, expected_dim in self.real_feature_dims.items():
                if feature_name not in features:
                    logger.error(f"âŒ [VALIDATION] ç¼ºå°‘ç‰¹å¾ç±»åˆ«: {feature_name}")
                    raise ValueError(f"ç¼ºå°‘ç‰¹å¾ç±»åˆ«: {feature_name}")
                
                actual_dim = features[feature_name].shape[1]
                if actual_dim != expected_dim:
                    logger.warning(f"âš ï¸ [VALIDATION] ç‰¹å¾ç»´åº¦ä¸åŒ¹é… {feature_name}: æœŸæœ›{expected_dim}, å®é™…{actual_dim}")
            
            logger.info(" [VALIDATION] æ—§æ ¼å¼éªŒè¯é€šè¿‡ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰")
            return True
        
        else:
            logger.error("âŒ [VALIDATION] Step1è¾“å‡ºæ ¼å¼æ— æ•ˆï¼šæ—¢æ²¡æœ‰MainPipelineæ ¼å¼ä¹Ÿæ²¡æœ‰æ—§ç‰¹å¾æ ¼å¼")
            logger.error(f"âŒ [VALIDATION] å¯ç”¨é”®: {list(result.keys())}")
            raise ValueError("Step1è¾“å‡ºæ ¼å¼æ— æ•ˆï¼šç¼ºå°‘å¿…è¦çš„ç‰¹å¾å­—æ®µ")
    
    def _convert_to_json_serializable(self, obj: Any) -> Any:
        """è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, torch.Tensor):
            return obj.cpu().tolist()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif hasattr(obj, '__dict__') and hasattr(obj, 'NodeID'):  # Nodeå¯¹è±¡
            # å°†Nodeå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            return {
                'NodeID': getattr(obj, 'NodeID', getattr(obj, 'node_id', 'Unknown')),
                'ShardID': getattr(obj, 'ShardID', getattr(obj, 'shard_id', 'Unknown')),
                'type': 'Node'
            }
        elif isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, (np.int32, np.int64, np.float32, np.float64)):
            return float(obj)
        elif hasattr(obj, 'item') and hasattr(obj, 'shape') and obj.shape == ():  # åªå¤„ç†numpyæ ‡é‡
            return obj.item()
        else:
            return obj


def create_blockemulator_integration_interface():
    """åˆ›å»ºBlockEmulatoré›†æˆæ¥å£"""
    logger.info("åˆ›å»ºBlockEmulatoré›†æˆæ¥å£")
    
    try:
        from blockemulator_integration_interface import BlockEmulatorIntegrationInterface
        return BlockEmulatorIntegrationInterface()
    except ImportError:
        logger.warning("ğŸ”Œ [INTEGRATION] BlockEmulatoré›†æˆæ¥å£ä¸å¯ç”¨")
        logger.warning("ğŸ”Œ [INTEGRATION] è¿™æ˜¯æ­£å¸¸çš„ç‹¬ç«‹è¿è¡Œæ¨¡å¼ï¼Œåˆ†ç‰‡ç»“æœå°†ä¿å­˜åˆ°æ–‡ä»¶")
        logger.warning("ğŸ”Œ [INTEGRATION] å¦‚éœ€é›†æˆåˆ°BlockEmulatorï¼Œè¯·ç¡®ä¿blockemulator_integration_interface.pyå¯ç”¨")
        
        class MockIntegrationInterface:
            def apply_sharding_to_blockemulator(self, sharding_config):
                logger.info("ğŸ”Œ [INTEGRATION] ç‹¬ç«‹æ¨¡å¼ï¼šåˆ†ç‰‡é…ç½®å·²å‡†å¤‡å°±ç»ª")
                logger.info("ğŸ”Œ [INTEGRATION] åˆ†ç‰‡ç»“æœå·²ä¿å­˜ï¼Œå¯æ‰‹åŠ¨åº”ç”¨åˆ°BlockEmulatorç³»ç»Ÿ")
                return {'status': 'simulated', 'config_applied': True}
        
        return MockIntegrationInterface()


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=== å¯åŠ¨å®Œæ•´é›†æˆåŠ¨æ€åˆ†ç‰‡ç³»ç»Ÿ ===")
    
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        sharding_system = CompleteIntegratedShardingSystem()
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        sharding_system.initialize_all_components()
        
        # è¿è¡Œå®Œæ•´æµæ°´çº¿
        pipeline_result = sharding_system.run_complete_pipeline()
        
        if pipeline_result['success']:
            # é›†æˆåˆ°BlockEmulator
            integration_result = sharding_system.integrate_with_blockemulator(pipeline_result)
            
            # åˆ›å»ºBlockEmulatoræ¥å£
            integration_interface = create_blockemulator_integration_interface()
            
            # åº”ç”¨åˆ†ç‰‡é…ç½®
            if hasattr(integration_interface, 'apply_sharding_to_blockemulator'):
                apply_result = integration_interface.apply_sharding_to_blockemulator(
                    integration_result.get('sharding_config', {})
                )
                logger.info(f"åˆ†ç‰‡é…ç½®åº”ç”¨ç»“æœ: {apply_result}")
            
            logger.info("å®Œæ•´é›†æˆåŠ¨æ€åˆ†ç‰‡ç³»ç»Ÿè¿è¡ŒæˆåŠŸï¼")
            logger.info("ç³»ç»Ÿå·²å‡†å¤‡å¥½æ¥å…¥BlockEmulator")
            
            # æ‰“å°å…³é”®ä¿¡æ¯
            print("\n=== ç³»ç»Ÿè¿è¡Œæ‘˜è¦ ===")
            print(f"ç®—æ³•: {pipeline_result.get('algorithm', 'Unknown')}")
            print(f"ç‰¹å¾æ•°é‡: {pipeline_result.get('feature_count', 'Unknown')}")
            print(f"åˆ†ç‰‡æ•°é‡: {pipeline_result.get('num_shards', 'Unknown')}")
            print(f"æ€§èƒ½è¯„åˆ†: {pipeline_result.get('performance_score', 'Unknown')}")
            print(f"æ‰§è¡Œæ—¶é—´: {pipeline_result.get('execution_time', 0):.2f}ç§’")
            print(f"è®¤è¯: çœŸå®40å­—æ®µ + å¤šå°ºåº¦å¯¹æ¯”å­¦ä¹  + EvolveGCN + ç»Ÿä¸€åé¦ˆ")
            
        else:
            logger.error("å®Œæ•´é›†æˆåŠ¨æ€åˆ†ç‰‡ç³»ç»Ÿè¿è¡Œå¤±è´¥")
            print(f"é”™è¯¯: {pipeline_result.get('error', 'Unknown error')}")
            
    except Exception as e:
        logger.error(f"ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        print(f"ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
