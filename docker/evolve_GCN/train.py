"""
ä¸»è®­ç»ƒè„šæœ¬ - EvolveGCNåŠ¨æ€åˆ†ç‰‡è®­ç»ƒ
"""
import os
import pickle
import torch
import torch.optim as optim
import numpy as np
import random

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from utils import get_device, print_device_info, HyperparameterUpdater
from models import EvolveGCNWrapper, DynamicShardingModule
from data import BlockchainDataset
from losses import multi_objective_sharding_loss, temporal_consistency_loss
from config import default_config

def set_random_seed(seed=42):
    """è®¾ç½®å…¨å±€éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class ShardingTrainer:
    """åŠ¨æ€åˆ†ç‰‡è®­ç»ƒå™¨"""

    def __init__(self, config=None):
        self.config = config or default_config
        self.device = get_device()

        # åˆå§‹åŒ–ç»„ä»¶
        self.dataset = None
        self.model = None
        self.sharding_module = None
        self.model_optimizer = None
        self.shard_optimizer = None
        self.param_updater = HyperparameterUpdater()

        # è®­ç»ƒçŠ¶æ€
        self.history_states = []
        self.prev_shard_assignment = None
        self.prev_shard_count = self.config.base_shards
        self.cross_increase_count = 0
        self.prev_cross_rate = 0.0

    def setup(self):
        """åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ"""
        print("å¼€å§‹EvolveGCNåŠ¨æ€åˆ†ç‰‡è®­ç»ƒ...")
        print_device_info()
        self.config.print_config()

        # 1. æ•°æ®å‡†å¤‡
        print("\nå‡†å¤‡æ•°æ®é›†...")
        self.dataset = BlockchainDataset(
            self.config.embedding_path,
            self.config.edge_index_path,
            num_timesteps=self.config.num_timesteps,
            noise_level=self.config.noise_level
        )

        input_dim = self.dataset.embedding_dim

        # 2. æ¨¡å‹åˆå§‹åŒ–
        print("\nåˆå§‹åŒ–æ¨¡å‹...")
        self.model = EvolveGCNWrapper(input_dim, self.config.hidden_dim).to(self.device)
        self.sharding_module = DynamicShardingModule(
            embedding_dim=self.config.hidden_dim,
            base_shards=self.config.base_shards,
            max_shards=self.config.max_shards,
            min_shard_size=self.config.min_shard_size,
            max_empty_ratio=self.config.max_empty_ratio
        ).to(self.device)

        # 3. ä¼˜åŒ–å™¨
        self.model_optimizer = optim.Adam(
            self.model.parameters(),
            lr=self.config.lr,
            weight_decay=self.config.weight_decay
        )
        self.shard_optimizer = optim.Adam(
            self.sharding_module.parameters(),
            lr=self.config.shard_lr
        )

        print(f"åˆå§‹åŒ–å®Œæˆ: è¾“å…¥ç»´åº¦={input_dim}, éšè—ç»´åº¦={self.config.hidden_dim}")

    def train_single_epoch(self, epoch):
        """å•ä¸ªepochçš„è®­ç»ƒ"""
        print(f"\n{'=' * 60}")
        print(f"Epoch {epoch + 1}/{self.config.epochs}")
        print(f"{'=' * 60}")

        # é‡ç½®æ¨¡å‹çŠ¶æ€
        self.model.reset_state()

        epoch_losses = {'gcn': 0.0, 'shard': 0.0, 'total': 0.0}
        all_embeddings = []
        all_delta_signals = []

        # å‡†å¤‡æ€§èƒ½åé¦ˆä¿¡å·
        performance_feedback = self._get_performance_feedback()

        # æ—¶åºå¤„ç†
        for t in range(self.config.num_timesteps):
            node_features, edge_index, timestep = self.dataset[t]
            node_features = node_features.to(self.device)
            edge_index = edge_index.to(self.device)

            # EvolveGCNå‰å‘ä¼ æ’­
            embeddings, delta_signal = self.model(node_features, edge_index, performance_feedback)
            all_embeddings.append(embeddings)
            all_delta_signals.append(delta_signal)

            # è®¡ç®—GCNæŸå¤±
            gcn_loss = self._compute_gcn_loss(embeddings, all_embeddings, t)
            epoch_losses['gcn'] += gcn_loss.item()

            # è¿›åº¦è¾“å‡º
            if t % 3 == 0:
                print(f"  æ—¶é—´æ­¥ {t}: åµŒå…¥å½¢çŠ¶ {embeddings.shape}, æŸå¤± {gcn_loss.item():.4f}")

        # åˆ†ç‰‡å†³ç­–
        shard_loss, loss_components, predicted_shards = self._compute_sharding_loss(
            all_embeddings[-1], epoch_losses
        )

        # æ€»æŸå¤±è®¡ç®—å’Œåå‘ä¼ æ’­
        total_loss = self._compute_total_loss(all_embeddings) + shard_loss
        epoch_losses['total'] = total_loss.item()

        self._backward_and_optimize(total_loss)

        # æ›´æ–°å†å²çŠ¶æ€
        self._update_history_states(loss_components, predicted_shards)

        # è¾“å‡ºè®­ç»ƒè¿›åº¦
        if epoch % self.config.print_freq == 0:
            self._print_epoch_summary(epoch, epoch_losses, loss_components, predicted_shards)

        return epoch_losses

    def _load_step4_feedback(self):
        """åŠ è½½ç¬¬å››æ­¥æ€§èƒ½åé¦ˆä¿¡å·"""
        try:
            from pathlib import Path
            import pickle
            
            # ç¬¬å››æ­¥åé¦ˆæ–‡ä»¶çš„å¯èƒ½ä½ç½®
            feedback_paths = [
                Path("../feedback/step4_feedback_result.pkl"),
                Path("feedback/step4_feedback_result.pkl"), 
                Path("step4_feedback_result.pkl"),
                Path("../feedback/step3_performance_feedback.pkl"),
                Path("step3_performance_feedback.pkl")
            ]
            
            for feedback_path in feedback_paths:
                if feedback_path.exists():
                    with open(feedback_path, "rb") as f:
                        feedback_data = pickle.load(f)
                    
                    print(f"   [SUCCESS] åŠ è½½ç¬¬å››æ­¥åé¦ˆ: {feedback_path}")
                    
                    # ï¿½ å¤„ç†ä¸åŒæ ¼å¼çš„åé¦ˆæ•°æ®
                    if isinstance(feedback_data, dict):
                        # æ–°æ ¼å¼ï¼šç›´æ¥åŒ…å«åé¦ˆçŸ©é˜µ
                        if 'step3_feedback' in feedback_data and 'assignment_guidance' in feedback_data['step3_feedback']:
                            feedback_matrix = feedback_data['step3_feedback']['assignment_guidance']
                            if isinstance(feedback_matrix, torch.Tensor):
                                return feedback_matrix.to(self.device)
                        
                        # æ—§æ ¼å¼ï¼šéœ€è¦æ„å»ºåé¦ˆçŸ©é˜µ
                        elif 'temporal_performance' in feedback_data:
                            # åŸºäºæ€§èƒ½å‘é‡æ„å»ºç®€å•åé¦ˆ
                            num_nodes = self.dataset.num_nodes if hasattr(self.dataset, 'num_nodes') else 100
                            performance_score = feedback_data['temporal_performance'].get('combined_score', 0.5)
                            
                            # æ ¹æ®æ€§èƒ½åˆ†æ•°è°ƒæ•´åˆ†ç‰‡å€¾å‘æ€§
                            if hasattr(self, 'prev_shard_assignment') and self.prev_shard_assignment is not None:
                                prev_shards = self.prev_shard_assignment.size(1)
                                feedback_matrix = torch.ones(num_nodes, prev_shards, device=self.device) / prev_shards
                                
                                # å¦‚æœæ€§èƒ½è¾ƒå·®ï¼Œé¼“åŠ±é‡æ–°åˆ†é…
                                if performance_score < 0.7:
                                    # æ·»åŠ ä¸€äº›éšæœºæ‰°åŠ¨é¼“åŠ±æ¢ç´¢
                                    noise = torch.randn_like(feedback_matrix) * 0.1
                                    feedback_matrix = torch.softmax(feedback_matrix + noise, dim=1)
                                
                                return feedback_matrix
                    
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åé¦ˆæ–‡ä»¶ï¼Œè¿”å›None
            return None
            
        except Exception as e:
            print(f"   [WARNING]  åŠ è½½ç¬¬å››æ­¥åé¦ˆå¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"[WARNING] ç¬¬å››æ­¥åé¦ˆåŠ è½½å¤±è´¥: {e}")
            return None
    
    def _get_performance_feedback(self):
        """è·å–æ€§èƒ½åé¦ˆä¿¡å· - ä¼˜å…ˆä½¿ç”¨ç¬¬å››æ­¥åé¦ˆ"""
        
        # [FIX] ä¼˜å…ˆå°è¯•ç¬¬å››æ­¥åé¦ˆ
        step4_feedback = self._load_step4_feedback()
        if step4_feedback is not None:
            print(f"[DATA] ä½¿ç”¨ç¬¬å››æ­¥åé¦ˆï¼Œç»´åº¦: {step4_feedback.shape}")
            return step4_feedback
        
        # åŸæœ‰çš„å†å²çŠ¶æ€åé¦ˆä½œä¸ºå¤‡é€‰
        if self.history_states:
            recent_states = torch.stack(self.history_states[-3:]) if len(self.history_states) >= 3 else torch.stack(
                self.history_states)
            performance_feedback = torch.mean(recent_states, dim=0).float().to(self.device)
            print(f"[DATA] ä½¿ç”¨å†å²çŠ¶æ€åé¦ˆï¼Œç»´åº¦: {performance_feedback.shape}")
        else:
            # [FIX] ä½¿ç”¨11ç»´é»˜è®¤åé¦ˆåŒ¹é…ç¬¬å››æ­¥æ ¼å¼
            performance_feedback = torch.tensor([
                0.5, 0.1, 0.8, 0.6,  # æ ¸å¿ƒ4ç»´ï¼šè´Ÿè½½å‡è¡¡, è·¨ç‰‡ç‡, å®‰å…¨æ€§, ç‰¹å¾è´¨é‡
                0.7, 0.6, 0.8, 0.5, 0.6, 0.7,  # 6ç»´ç‰¹å¾è´¨é‡
                0.65  # ç»¼åˆåˆ†æ•°
            ], dtype=torch.float32, device=self.device)
            print(f"[DATA] ä½¿ç”¨é»˜è®¤åé¦ˆï¼Œç»´åº¦: {performance_feedback.shape}")
        
        return performance_feedback
    def _compute_gcn_loss(self, embeddings, all_embeddings, t):
        """è®¡ç®—GCNæŸå¤±"""
        gcn_loss = torch.tensor(0.0, dtype=torch.float32, device=self.device)

        # æ—¶åºä¸€è‡´æ€§æŸå¤±
        if t > 0:
            consistency_loss = temporal_consistency_loss(
                embeddings, all_embeddings[t - 1].detach(),
                lambda_contrast=self.param_updater.params['lambda']
            )
            gcn_loss += consistency_loss

        # æ­£åˆ™åŒ–æŸå¤±
        reg_loss = 0.01 * torch.mean(torch.norm(embeddings, p=2, dim=1))
        gcn_loss += reg_loss

        return gcn_loss

    def _compute_sharding_loss(self, final_embeddings, epoch_losses):
        """è®¡ç®—åˆ†ç‰‡æŸå¤± - é›†æˆç¬¬å››æ­¥åé¦ˆ"""
        # ğŸ”„ åŠ è½½ç¬¬å››æ­¥åé¦ˆä¿¡å·
        feedback_signal = self._load_step4_feedback()
        
        # åŠ¨æ€åˆ†ç‰‡ - ä½¿ç”¨ä¼˜åŒ–åçš„æ¨¡å—
        shard_assignment, enhanced_embeddings, attention_weights, predicted_shards = self.sharding_module(
            Z=final_embeddings, 
            history_states=self.history_states, 
            feedback_signal=feedback_signal
        )

        print(f" é¢„æµ‹åˆ†ç‰‡æ•°: {predicted_shards} (ä¸Šä¸€è½®: {self.prev_shard_count})")
        if feedback_signal is not None:
            print(f" ä½¿ç”¨ç¬¬å››æ­¥åé¦ˆ: {feedback_signal.shape}")

        # ç”Ÿæˆå®‰å…¨è¯„åˆ†ï¼ˆæ¨¡æ‹Ÿï¼‰
        security_scores = torch.rand(final_embeddings.size(0), dtype=torch.float32, device=self.device) * 0.5

        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å†å²åˆ†é…
        use_prev_assignment = (self.prev_shard_assignment is not None and
                               self.prev_shard_assignment.size(1) == shard_assignment.size(1))

        # è®¡ç®—åˆ†ç‰‡æŸå¤±
        shard_loss, loss_components = multi_objective_sharding_loss(
            shard_assignment, enhanced_embeddings, self.dataset.edge_index.to(self.device),
            prev_assignment=self.prev_shard_assignment if use_prev_assignment else None,
            security_scores=security_scores,
            a=self.param_updater.params.get('balance_weight', 1.0),
            b=self.param_updater.params.get('cross_weight', 1.0),
            c=self.param_updater.params.get('security_weight', 1.5),
            d=self.param_updater.params.get('migrate_weight', 0.5) if use_prev_assignment else 0.0
        )

        epoch_losses['shard'] = shard_loss.item()

        # ä¿å­˜å½“å‰åˆ†é…
        self.prev_shard_assignment = shard_assignment.detach().clone()
        self.prev_shard_count = predicted_shards

        return shard_loss, loss_components, predicted_shards

    def _compute_total_loss(self, all_embeddings):
        """è®¡ç®—æ€»çš„GCNæŸå¤±"""
        gcn_total_loss = torch.tensor(0.0, dtype=torch.float32, device=self.device)
        for i, emb in enumerate(all_embeddings):
            if i > 0:
                consistency_loss = temporal_consistency_loss(
                    emb, all_embeddings[i - 1].detach(),
                    lambda_contrast=self.param_updater.params['lambda']
                )
                gcn_total_loss += consistency_loss

            reg_loss = 0.01 * torch.mean(torch.norm(emb, p=2, dim=1))
            gcn_total_loss += reg_loss

        return gcn_total_loss

    def _backward_and_optimize(self, total_loss):
        """åå‘ä¼ æ’­å’Œä¼˜åŒ–"""
        self.model_optimizer.zero_grad()
        self.shard_optimizer.zero_grad()
        total_loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.config.max_grad_norm)
        torch.nn.utils.clip_grad_norm_(self.sharding_module.parameters(), max_norm=self.config.max_grad_norm)

        self.model_optimizer.step()
        self.shard_optimizer.step()

    def _update_history_states(self, loss_components, predicted_shards):
        """æ›´æ–°å†å²çŠ¶æ€"""
        with torch.no_grad():
            hard_assignment = torch.argmax(self.prev_shard_assignment, dim=1)
            shard_sizes = [(hard_assignment == s).sum().item() for s in range(predicted_shards)]

            # æ€§èƒ½æŒ‡æ ‡è®¡ç®—
            balance_score = 1.0 - (np.std(shard_sizes) / (np.mean(shard_sizes) + 1e-8))
            cross_rate = loss_components['cross']
            security_score = 1.0 - loss_components['security']

            # è·¨ç‰‡äº¤æ˜“ç‡è¶‹åŠ¿æ£€æµ‹
            if cross_rate > self.prev_cross_rate:
                self.cross_increase_count += 1
            else:
                self.cross_increase_count = 0
            self.prev_cross_rate = cross_rate

            # æ›´æ–°å†å²çŠ¶æ€
            current_state = torch.tensor([balance_score, cross_rate, security_score],
                                         dtype=torch.float32, device=self.device)
            self.history_states.append(current_state)
            if len(self.history_states) > self.config.history_length:
                self.history_states.pop(0)

            # æ›´æ–°çŠ¶æ€è·Ÿè¸ª
            self.prev_shard_count = predicted_shards

            # åŠ¨æ€è°ƒèŠ‚å‚æ•°
            performance_metrics = {
                'balance_score': balance_score,
                'cross_tx_rate': cross_rate,
                'cross_increase_count': self.cross_increase_count
            }
            self.param_updater.update_hyperparams(performance_metrics)

    def _print_epoch_summary(self, epoch, epoch_losses, loss_components, predicted_shards):
        """æ‰“å°epochæ€»ç»“"""
        hard_assignment = torch.argmax(self.prev_shard_assignment, dim=1)
        shard_sizes = [(hard_assignment == s).sum().item() for s in range(predicted_shards)]

        balance_score = 1.0 - (np.std(shard_sizes) / (np.mean(shard_sizes) + 1e-8))
        cross_rate = loss_components['cross']
        security_score = 1.0 - loss_components['security']

        print(f"\nEpoch {epoch + 1} æ€»ç»“:")
        print(f"  æ€»æŸå¤±: {epoch_losses['total']:.4f}")
        print(f"  GCNæŸå¤±: {epoch_losses['gcn']:.4f}")
        print(f"  åˆ†ç‰‡æŸå¤±: {epoch_losses['shard']:.4f}")
        print(f"  æŸå¤±ç»„ä»¶: {loss_components}")
        print(f"  åˆ†ç‰‡å¤§å°: {shard_sizes}")
        print(f"  æ€§èƒ½æŒ‡æ ‡: å‡è¡¡={balance_score:.3f}, è·¨ç‰‡={cross_rate:.3f}, å®‰å…¨={security_score:.3f}")

        updated_params = self.param_updater.get_params()
        print(
            f"  åŠ¨æ€å‚æ•°: Î±={updated_params['alpha']:.3f}, Î»={updated_params['lambda']:.3f}, K={updated_params['K_base']}")

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.setup()

        print(f"\nå¼€å§‹è®­ç»ƒ {self.config.epochs} ä¸ªepochs...")

        for epoch in range(self.config.epochs):
            epoch_losses = self.train_single_epoch(epoch)

        print("\nè®­ç»ƒå®Œæˆ!")
        return self._generate_final_results()

    def _generate_final_results(self):
        """ç”Ÿæˆæœ€ç»ˆç»“æœ"""
        print("ç”Ÿæˆæœ€ç»ˆåµŒå…¥...")
        self.model.eval()
        self.model.reset_state()
        new_embeddings = {}

        with torch.no_grad():
            for t in range(self.config.num_timesteps):
                node_features, edge_index, timestep = self.dataset[t]
                embeddings, _ = self.model(node_features.to(self.device), edge_index.to(self.device))
                new_embeddings[str(t)] = embeddings.cpu().numpy()

        return self._save_results(new_embeddings)

    def _save_results(self, new_embeddings):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        print(" ä¿å­˜ç»“æœ...")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config.output_dir, exist_ok=True)
        os.makedirs(self.config.model_dir, exist_ok=True)

        # ä¿å­˜åµŒå…¥
        embedding_path = os.path.join(self.config.output_dir, 'new_temporal_embeddings.pkl')
        with open(embedding_path, 'wb') as f:
            pickle.dump(new_embeddings, f)

        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(self.config.model_dir, 'enhanced_evolvegcn_model.pth')
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'sharding_state_dict': self.sharding_module.state_dict(),
            'history_states': self.history_states,
            'hyperparams': self.param_updater.get_params(),
            'config': self.config.to_dict()
        }, model_path)

        # ç”Ÿæˆæœ€ç»ˆåˆ†ç‰‡ç»“æœ
        sharding_results = self._generate_sharding_results(new_embeddings)
        shard_path = os.path.join(self.config.output_dir, 'sharding_results.pkl')
        with open(shard_path, 'wb') as f:
            pickle.dump(sharding_results, f)

        print("æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        print(f"è¾“å‡ºæ–‡ä»¶:")
        print(f"  - åµŒå…¥ç»“æœ: {embedding_path}")
        print(f"  - è®­ç»ƒæ¨¡å‹: {model_path}")
        print(f"  - åˆ†ç‰‡ç»“æœ: {shard_path}")

        # æ‰“å°åˆ†ç‰‡ç»“æœç»Ÿè®¡
        for key, value in sharding_results.items():
            print(f"  - {key}: {len(value)} èŠ‚ç‚¹ {value}")

        return new_embeddings, sharding_results, self.history_states

    def _generate_sharding_results(self, new_embeddings):
        """ç”Ÿæˆåˆ†ç‰‡ç»“æœ"""
        final_embeddings = torch.tensor(
            new_embeddings[str(self.config.num_timesteps - 1)],
            dtype=torch.float32, device=self.device
        )
        final_assignment, _, _, final_shards = self.sharding_module(final_embeddings, self.history_states)
        hard_assignment = torch.argmax(final_assignment, dim=1)

        sharding_results = {}
        for s in range(final_shards):
            shard_nodes = (hard_assignment == s).nonzero(as_tuple=True)[0].cpu().numpy()
            sharding_results[f'shard_{s}'] = shard_nodes.tolist()

        return sharding_results


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
        trainer = ShardingTrainer(default_config)

        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
        set_random_seed(42)
        embeddings, sharding, history = trainer.train()

        print("è®­ç»ƒæˆåŠŸå®Œæˆï¼")

        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print(f"\næœ€ç»ˆç»Ÿè®¡:")
        print(f"  ç”ŸæˆåµŒå…¥: {len(embeddings)} ä¸ªæ—¶é—´æ­¥")
        print(f"  åˆ†ç‰‡ç»“æœ: {len(sharding)} ä¸ªåˆ†ç‰‡")
        print(f"  å†å²çŠ¶æ€: {len(history)} ä¸ªè®°å½•")

        return embeddings, sharding, history

    except Exception as e:
        print(f"[ERROR] è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None


if __name__ == "__main__":
    main()